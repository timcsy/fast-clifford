"""
CGA0D Cl(1,1) Functional Operations - Auto-generated

DO NOT EDIT MANUALLY - This file is generated by codegen/generate.py

Generated: 2025-12-09 02:12:46
Algebra: cga0d
Signature: (1, -1)
Blade count: 4

All functions are:
- Loop-free (fully expanded arithmetic)
- ONNX-compatible (only Add/Mul/Neg/Sub operations)
- Hard-coded (no Cayley table lookups)
"""

import torch
from torch import Tensor
from typing import Tuple


# =============================================================================
# Constants
# =============================================================================

BLADE_COUNT = 4
EUCLIDEAN_DIM = 0

# Blade indices by grade
GRADE_0_INDICES = (0,)
GRADE_1_INDICES = (1, 2)
GRADE_2_INDICES = (3,)

# Sparsity masks
POINT_MASK = (1, 2)  # 2 components
EVEN_VERSOR_MASK = (0, 3)  # 2 components

# Reverse signs for all 4 blades
REVERSE_SIGNS = (1, 1, 1, -1)

# EvenVersor-specific reverse signs (2 components)
EVEN_VERSOR_REVERSE_SIGNS = (1, -1)

# =============================================================================
# Geometric Product (Full 4x4)
# =============================================================================

@torch.jit.script
def geometric_product_full(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute the full geometric product of two multivectors.

    Args:
        a: Left operand, shape (..., 4)
        b: Right operand, shape (..., 4)

    Returns:
        Result multivector, shape (..., 4)

    Note:
        Fully expanded, no loops, ONNX compatible.
    """
    r0 = (
        a[..., 0] * b[..., 0] +
        a[..., 1] * b[..., 1] +
        -a[..., 2] * b[..., 2] +
        a[..., 3] * b[..., 3]
    )
    r1 = (
        a[..., 0] * b[..., 1] +
        a[..., 1] * b[..., 0] +
        a[..., 2] * b[..., 3] +
        -a[..., 3] * b[..., 2]
    )
    r2 = (
        a[..., 0] * b[..., 2] +
        a[..., 1] * b[..., 3] +
        a[..., 2] * b[..., 0] +
        -a[..., 3] * b[..., 1]
    )
    r3 = (
        a[..., 0] * b[..., 3] +
        a[..., 1] * b[..., 2] +
        -a[..., 2] * b[..., 1] +
        a[..., 3] * b[..., 0]
    )

    return torch.stack([
        r0, r1, r2, r3,
    ], dim=-1)

# =============================================================================
# Reverse Operation
# =============================================================================

@torch.jit.script
def reverse_full(mv: Tensor) -> Tensor:
    """
    Compute the reverse of a multivector.

    For grade k: coefficient *= (-1)^(k*(k-1)/2)

    Args:
        mv: Input multivector, shape (..., 4)

    Returns:
        Reversed multivector, shape (..., 4)
    """
    r0 = mv[..., 0]
    r1 = mv[..., 1]
    r2 = mv[..., 2]
    r3 = -mv[..., 3]

    return torch.stack([
        r0, r1, r2, r3,
    ], dim=-1)

# =============================================================================
# Sparse Operations (EvenVersor[2] x Point[2])
# =============================================================================

# EvenVersor sparse indices: (0, 3)
# Point sparse indices: (1, 2)

@torch.jit.script
def cga_encode(x: Tensor) -> Tensor:
    """
    Encode 0D vector to UPGC point representation.

    X = n_o + x + 0.5|x|^2 * n_inf

    Where:
        n_o = 0.5 * (e- - e+)   -> coefficients: e+ = -0.5, e- = 0.5
        n_inf = e- + e+         -> coefficients: e+ = 1, e- = 1

    Args:
        x: 0D vector, shape (..., 0)

    Returns:
        UPGC point, shape (..., 2)
    """
    # CGA0D: no euclidean components
    # x is expected to be shape (..., 0) - use for batch shape
    batch_shape = x.shape[:-1]
    r0 = torch.full(batch_shape, -0.5, dtype=x.dtype, device=x.device)  # e+
    r1 = torch.full(batch_shape, 0.5, dtype=x.dtype, device=x.device)  # e-

    return torch.stack([r0, r1], dim=-1)


@torch.jit.script
def cga_decode(point: Tensor) -> Tensor:
    """
    Decode UPGC point to 0D vector.

    Extracts the first 0 components (euclidean part).

    Args:
        point: UPGC point, shape (..., 2)

    Returns:
        0D vector, shape (..., 0)
    """
    return point[..., :0]


@torch.jit.script
def reverse_even_versor(ev: Tensor) -> Tensor:
    """
    Compute reverse of an EvenVersor (sparse 2-component version).

    Args:
        ev: EvenVersor, shape (..., 2)

    Returns:
        Reversed EvenVersor, shape (..., 2)
    """
    r0 = ev[..., 0]  # 1: keep
    r1 = -ev[..., 1]  # e+e-: negate

    return torch.stack([r0, r1], dim=-1)


@torch.jit.script
def sandwich_product_sparse(ev: Tensor, point: Tensor) -> Tensor:
    """
    Compute sparse sandwich product: M × X × M̃

    Optimized for:
        - EvenVersor M: 2 components
        - Point X: 2 components
        - Output: 2 components

    Args:
        ev: EvenVersor, shape (..., 2)
        point: UPGC point, shape (..., 2)

    Returns:
        Transformed point, shape (..., 2)

    Note:
        Total multiplications: 16 (vs 16 for naive)
    """
    # EvenVersor components (sparse)
    m0 = ev[..., 0]  # 1
    m1 = ev[..., 1]  # e+e-

    # Point components (sparse)
    p0 = point[..., 0]  # e+
    p1 = point[..., 1]  # e-

    # EvenVersor reverse
    mr0 = m0
    mr1 = -m1

    # Output r0 (e+): 4 terms
    r0 = (
        m0 * p0 * mr0 +
        m0 * p1 * mr1 +
        -m1 * p0 * mr1 +
        -m1 * p1 * mr0
    )

    # Output r1 (e-): 4 terms
    r1 = (
        m0 * p0 * mr1 +
        m0 * p1 * mr0 +
        -m1 * p0 * mr0 +
        -m1 * p1 * mr1
    )

    return torch.stack([r0, r1], dim=-1)


# =============================================================================
# EvenVersor Composition
# =============================================================================

@torch.jit.script
def compose_even_versor(v1: Tensor, v2: Tensor) -> Tensor:
    """
    Compose two EvenVersors via geometric product.

    Args:
        v1: First EvenVersor, shape (..., 2)
        v2: Second EvenVersor, shape (..., 2)

    Returns:
        Composed EvenVersor, shape (..., 2)
    """
    # v1 components
    v1_0 = v1[..., 0]  # 1
    v1_1 = v1[..., 1]  # e+e-

    # v2 components
    v2_0 = v2[..., 0]  # 1
    v2_1 = v2[..., 1]  # e+e-

    # r0 (1): 2 terms
    r0 = v1_0 * v2_0 + v1_1 * v2_1
    # r1 (e+e-): 2 terms
    r1 = v1_0 * v2_1 + v1_1 * v2_0

    return torch.stack([r0, r1], dim=-1)

# =============================================================================
# Similitude Composition (CGA-specific accelerated)
# =============================================================================

@torch.jit.script
def compose_similitude(s1: Tensor, s2: Tensor) -> Tensor:
    """
    Compose two Similitudes via optimized geometric product.

    Similitude constraint: ei+ = ei- (translation components equal)
    This allows 30-50% speedup over general EvenVersor composition.

    Args:
        s1: First Similitude, shape (..., 2)
        s2: Second Similitude, shape (..., 2)

    Returns:
        Composed Similitude, shape (..., 2)
    """
    # Note: Currently same as compose_even_versor
    # Future optimization: exploit ei+ = ei- constraint
    # s1 components
    s1_0 = s1[..., 0]  # 1
    s1_1 = s1[..., 1]  # e+e-

    # s2 components
    s2_0 = s2[..., 0]  # 1
    s2_1 = s2[..., 1]  # e+e-

    # r0 (1): 2 terms
    r0 = s1_0 * s2_0 + s1_1 * s2_1
    # r1 (e+e-): 2 terms
    r1 = s1_0 * s2_1 + s1_1 * s2_0

    return torch.stack([r0, r1], dim=-1)

# =============================================================================
# Similitude Sandwich Product (CGA-specific accelerated)
# =============================================================================

@torch.jit.script
def sandwich_product_similitude(similitude: Tensor, point: Tensor) -> Tensor:
    """
    Compute sandwich product S × X × ~S for Similitude.

    Faster than sandwich_product_sparse by exploiting Similitude constraints.

    Args:
        similitude: Similitude, shape (..., 2)
        point: UPGC point, shape (..., 2)

    Returns:
        Transformed point, shape (..., 2)
    """
    # Currently delegates to sandwich_product_sparse
    # Future optimization: exploit ei+ = ei- constraint
    return sandwich_product_sparse(similitude, point)

# =============================================================================
# Geometric Inner Product (Metric Inner Product)
# =============================================================================

@torch.jit.script
def inner_product_full(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute geometric inner product with CGA metric.

    Args:
        a: First multivector, shape (..., 4)
        b: Second multivector, shape (..., 4)

    Returns:
        Scalar inner product, shape (..., 1)

    Note:
        Uses fused sign flipping: res = sum(a[i] * b[i] * METRIC_SIGNS[i])
    """
    r = (
        a[..., 0] * b[..., 0] +  # 1² = 1
        a[..., 1] * b[..., 1] +  # e+² = 1
        -a[..., 2] * b[..., 2] +  # e-² = -1
        a[..., 3] * b[..., 3]  # e+e-² = 1
    )
    return r.unsqueeze(-1)

# =============================================================================
# Bivector Squared Scalar (Helper for exp_bivector)
# =============================================================================

@torch.jit.script
def bivector_squared_scalar(B: Tensor) -> Tensor:
    """
    Compute the scalar part of B².

    Args:
        B: Bivector, shape (..., 1)

    Returns:
        Scalar B² value, shape (...,)
    """
    r = (
        B[..., 0] * B[..., 0]
    )
    return r

# =============================================================================
# Bivector Exponential Map
# =============================================================================

@torch.jit.script
def exp_bivector(B: Tensor) -> Tensor:
    """
    Compute exponential map from Bivector to EvenVersor.

    exp(B) = cos(θ) + sinc(θ) * B
    where θ² = -B² (for rotation bivectors)

    Args:
        B: Bivector, shape (..., 1)

    Returns:
        EvenVersor, shape (..., 2)

    Note:
        Uses torch.sinc for numerical stability at θ→0
    """
    # Compute B² scalar
    B_sq = bivector_squared_scalar(B)

    # θ² = -B² (for rotation bivectors, B² is negative)
    theta_sq = torch.clamp(-B_sq, min=1e-12)
    theta = torch.sqrt(theta_sq)

    # cos(θ) and sinc(θ) = sin(θ)/θ
    cos_theta = torch.cos(theta)
    # torch.sinc(x) = sin(πx)/(πx), so sinc(θ/π) = sin(θ)/θ
    sinc_theta = torch.sinc(theta / 3.141592653589793)

    # Build even_versor: scalar part = cos(θ), bivector parts = sinc(θ) * B
    r0 = cos_theta  # 1
    r1 = sinc_theta * B[..., 0]  # e+e-

    return torch.stack([r0, r1], dim=-1)

# =============================================================================
# Outer Product (Wedge Product)
# =============================================================================

@torch.jit.script
def outer_product_full(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute outer product (wedge product).

    a ∧ b = <a * b>_{grade(a) + grade(b)}

    Args:
        a: First multivector, shape (..., 4)
        b: Second multivector, shape (..., 4)

    Returns:
        Wedge product, shape (..., 4)
    """
    r0 = a[..., 0] * b[..., 0]  # 1
    r1 = a[..., 0] * b[..., 1] + a[..., 1] * b[..., 0]  # e+
    r2 = a[..., 0] * b[..., 2] + a[..., 2] * b[..., 0]  # e-
    r3 = (  # e+e-
        a[..., 0] * b[..., 3] +
        a[..., 1] * b[..., 2] +
        -a[..., 2] * b[..., 1] +
        a[..., 3] * b[..., 0]
    )

    return torch.stack([
        r0, r1, r2, r3,
    ], dim=-1)

# =============================================================================
# Left Contraction
# =============================================================================

@torch.jit.script
def left_contraction_full(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute left contraction.

    a ⌋ b = <a * b>_{grade(b) - grade(a)} if grade(a) <= grade(b), else 0

    Args:
        a: First multivector, shape (..., 4)
        b: Second multivector, shape (..., 4)

    Returns:
        Left contraction result, shape (..., 4)
    """
    r0 = (  # 1
        a[..., 0] * b[..., 0] +
        a[..., 1] * b[..., 1] +
        -a[..., 2] * b[..., 2] +
        a[..., 3] * b[..., 3]
    )
    r1 = a[..., 0] * b[..., 1] + a[..., 2] * b[..., 3]  # e+
    r2 = a[..., 0] * b[..., 2] + a[..., 1] * b[..., 3]  # e-
    r3 = a[..., 0] * b[..., 3]  # e+e-

    return torch.stack([
        r0, r1, r2, r3,
    ], dim=-1)

# =============================================================================
# Right Contraction
# =============================================================================

@torch.jit.script
def right_contraction_full(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute right contraction.

    a ⌊ b = <a * b>_{grade(a) - grade(b)} if grade(a) >= grade(b), else 0

    Args:
        a: First multivector, shape (..., 4)
        b: Second multivector, shape (..., 4)

    Returns:
        Right contraction result, shape (..., 4)
    """
    r0 = (  # 1
        a[..., 0] * b[..., 0] +
        a[..., 1] * b[..., 1] +
        -a[..., 2] * b[..., 2] +
        a[..., 3] * b[..., 3]
    )
    r1 = a[..., 1] * b[..., 0] + -a[..., 3] * b[..., 2]  # e+
    r2 = a[..., 2] * b[..., 0] + -a[..., 3] * b[..., 1]  # e-
    r3 = a[..., 3] * b[..., 0]  # e+e-

    return torch.stack([
        r0, r1, r2, r3,
    ], dim=-1)

# =============================================================================
# Grade Selection
# =============================================================================

# Grade masks for grade selection
GRADE_0_MASK = (0,)
GRADE_1_MASK = (1, 2)
GRADE_2_MASK = (3,)

@torch.jit.script
def grade_select(mv: Tensor, grade: int) -> Tensor:
    """
    Extract components of a specific grade.

    Args:
        mv: Multivector, shape (..., 4)
        grade: Grade to extract (0 to 2)

    Returns:
        Grade components (zeros for non-selected grades)
    """
    if grade == 0:
        r0 = mv[..., 0]
        r1 = torch.zeros_like(mv[..., 0])
        r2 = torch.zeros_like(mv[..., 0])
        r3 = torch.zeros_like(mv[..., 0])
        return torch.stack([
            r0, r1, r2, r3,
        ], dim=-1)
    elif grade == 1:
        r0 = torch.zeros_like(mv[..., 0])
        r1 = mv[..., 1]
        r2 = mv[..., 2]
        r3 = torch.zeros_like(mv[..., 0])
        return torch.stack([
            r0, r1, r2, r3,
        ], dim=-1)
    elif grade == 2:
        r0 = torch.zeros_like(mv[..., 0])
        r1 = torch.zeros_like(mv[..., 0])
        r2 = torch.zeros_like(mv[..., 0])
        r3 = mv[..., 3]
        return torch.stack([
            r0, r1, r2, r3,
        ], dim=-1)
    else:
        return torch.zeros_like(mv)

# =============================================================================
# Dual Operation
# =============================================================================

# Pseudoscalar index: 3, I² = 1
# I^{-1} = I / I² = I * 1.0

@torch.jit.script
def dual(mv: Tensor) -> Tensor:
    """
    Compute the dual of a multivector.

    a* = a ⌋ I^{-1} (left contraction with pseudoscalar inverse)

    Args:
        mv: Multivector, shape (..., 4)

    Returns:
        Dual multivector, shape (..., 4)
    """
    r0 = mv[..., 3]  # 1
    r1 = mv[..., 2]  # e+
    r2 = mv[..., 1]  # e-
    r3 = mv[..., 0]  # e+e-

    return torch.stack([
        r0, r1, r2, r3,
    ], dim=-1)

# =============================================================================
# Normalize Operation
# =============================================================================

@torch.jit.script
def norm_squared(mv: Tensor) -> Tensor:
    """
    Compute squared norm of a multivector.

    |a|² = <a * ~a>_0

    Args:
        mv: Multivector, shape (..., 4)

    Returns:
        Squared norm, shape (...,)
    """
    r = (
        mv[..., 0] * mv[..., 0] +
        mv[..., 1] * mv[..., 1] +
        -mv[..., 2] * mv[..., 2] +
        -mv[..., 3] * mv[..., 3]
    )
    return r


@torch.jit.script
def normalize(mv: Tensor) -> Tensor:
    """
    Normalize a multivector to unit norm.

    Args:
        mv: Multivector, shape (..., 4)

    Returns:
        Normalized multivector, shape (..., 4)

    Note:
        Returns NaN for null multivectors (ONNX compatible)
    """
    norm = torch.sqrt(torch.abs(norm_squared(mv)) + 1e-12)
    return mv / norm.unsqueeze(-1)

# =============================================================================
# Structure Normalize (US9a)
# =============================================================================

ROTOR_INDICES: Tuple[int, ...] = (0,)
TRANSLATION_PAIRS: Tuple[Tuple[int, int], ...] = ()
DILATION_INDEX: int = 1


@torch.jit.script
def structure_normalize(similitude: Tensor, eps: float = 1e-8) -> Tensor:
    """
    Structure-normalize a Similitude tensor.

    Ensures:
    1. Rotor part (scalar + spatial bivectors) has unit norm
    2. Similitude constraint: eie+ = eie- (no transversion)

    Args:
        similitude: Similitude tensor, shape (..., 2)
        eps: Numerical stability constant

    Returns:
        Structure-normalized Similitude, shape (..., 2)
    """
    result = similitude.clone()

    # Step 1: Normalize Rotor part
    rotor_norm_sq = similitude[..., 0] * similitude[..., 0]
    rotor_norm = torch.sqrt(rotor_norm_sq + eps)
    result[..., 0] = similitude[..., 0] / rotor_norm

    # Step 2: Enforce Similitude constraint (eie+ = eie-)
    # No translation pairs for this dimension

    return result


@torch.jit.script
def soft_structure_normalize(similitude: Tensor, strength: float = 1.0, eps: float = 1e-8) -> Tensor:
    """
    Soft structure normalization with interpolation.

    Args:
        similitude: Similitude tensor, shape (..., 2)
        strength: Interpolation strength (0=no change, 1=full normalize)
        eps: Numerical stability constant

    Returns:
        Soft-normalized Similitude, shape (..., 2)
    """
    normalized = structure_normalize(similitude, eps)
    return similitude + strength * (normalized - similitude)
