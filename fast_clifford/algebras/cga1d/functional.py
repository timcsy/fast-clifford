"""
CGA1D Cl(2,1) Functional Operations - Auto-generated

DO NOT EDIT MANUALLY - This file is generated by codegen/generate.py

Generated: 2025-12-09 02:13:03
Algebra: cga1d
Signature: (1, 1, -1)
Blade count: 8

All functions are:
- Loop-free (fully expanded arithmetic)
- ONNX-compatible (only Add/Mul/Neg/Sub operations)
- Hard-coded (no Cayley table lookups)
"""

import torch
from torch import Tensor
from typing import Tuple


# =============================================================================
# Constants
# =============================================================================

BLADE_COUNT = 8
EUCLIDEAN_DIM = 1

# Blade indices by grade
GRADE_0_INDICES = (0,)
GRADE_1_INDICES = (1, 2, 3)
GRADE_2_INDICES = (4, 5, 6)
GRADE_3_INDICES = (7,)

# Sparsity masks
UPGC_POINT_MASK = (1, 2, 3)  # 3 components
EVEN_VERSOR_MASK = (0, 4, 5, 6)  # 4 components

# Reverse signs for all 8 blades
REVERSE_SIGNS = (1, 1, 1, 1, -1, -1, -1, -1)

# EvenVersor-specific reverse signs (4 components)
EVEN_VERSOR_REVERSE_SIGNS = (1, -1, -1, -1)

# =============================================================================
# Geometric Product (Full 8x8)
# =============================================================================

@torch.jit.script
def geometric_product_full(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute the full geometric product of two multivectors.

    Args:
        a: Left operand, shape (..., 8)
        b: Right operand, shape (..., 8)

    Returns:
        Result multivector, shape (..., 8)

    Note:
        Fully expanded, no loops, ONNX compatible.
    """
    r0 = (
        a[..., 0] * b[..., 0] +
        a[..., 1] * b[..., 1] +
        a[..., 2] * b[..., 2] +
        -a[..., 3] * b[..., 3] +
        -a[..., 4] * b[..., 4] +
        a[..., 5] * b[..., 5] +
        a[..., 6] * b[..., 6] +
        a[..., 7] * b[..., 7]
    )
    r1 = (
        a[..., 0] * b[..., 1] +
        a[..., 1] * b[..., 0] +
        -a[..., 2] * b[..., 4] +
        a[..., 3] * b[..., 5] +
        a[..., 4] * b[..., 2] +
        -a[..., 5] * b[..., 3] +
        a[..., 6] * b[..., 7] +
        a[..., 7] * b[..., 6]
    )
    r2 = (
        a[..., 0] * b[..., 2] +
        a[..., 1] * b[..., 4] +
        a[..., 2] * b[..., 0] +
        a[..., 3] * b[..., 6] +
        -a[..., 4] * b[..., 1] +
        -a[..., 5] * b[..., 7] +
        -a[..., 6] * b[..., 3] +
        -a[..., 7] * b[..., 5]
    )
    r3 = (
        a[..., 0] * b[..., 3] +
        a[..., 1] * b[..., 5] +
        a[..., 2] * b[..., 6] +
        a[..., 3] * b[..., 0] +
        -a[..., 4] * b[..., 7] +
        -a[..., 5] * b[..., 1] +
        -a[..., 6] * b[..., 2] +
        -a[..., 7] * b[..., 4]
    )
    r4 = (
        a[..., 0] * b[..., 4] +
        a[..., 1] * b[..., 2] +
        -a[..., 2] * b[..., 1] +
        -a[..., 3] * b[..., 7] +
        a[..., 4] * b[..., 0] +
        a[..., 5] * b[..., 6] +
        -a[..., 6] * b[..., 5] +
        -a[..., 7] * b[..., 3]
    )
    r5 = (
        a[..., 0] * b[..., 5] +
        a[..., 1] * b[..., 3] +
        -a[..., 2] * b[..., 7] +
        -a[..., 3] * b[..., 1] +
        a[..., 4] * b[..., 6] +
        a[..., 5] * b[..., 0] +
        -a[..., 6] * b[..., 4] +
        -a[..., 7] * b[..., 2]
    )
    r6 = (
        a[..., 0] * b[..., 6] +
        a[..., 1] * b[..., 7] +
        a[..., 2] * b[..., 3] +
        -a[..., 3] * b[..., 2] +
        -a[..., 4] * b[..., 5] +
        a[..., 5] * b[..., 4] +
        a[..., 6] * b[..., 0] +
        a[..., 7] * b[..., 1]
    )
    r7 = (
        a[..., 0] * b[..., 7] +
        a[..., 1] * b[..., 6] +
        -a[..., 2] * b[..., 5] +
        a[..., 3] * b[..., 4] +
        a[..., 4] * b[..., 3] +
        -a[..., 5] * b[..., 2] +
        a[..., 6] * b[..., 1] +
        a[..., 7] * b[..., 0]
    )

    return torch.stack([
        r0, r1, r2, r3, r4, r5, r6, r7,
    ], dim=-1)

# =============================================================================
# Reverse Operation
# =============================================================================

@torch.jit.script
def reverse_full(mv: Tensor) -> Tensor:
    """
    Compute the reverse of a multivector.

    For grade k: coefficient *= (-1)^(k*(k-1)/2)

    Args:
        mv: Input multivector, shape (..., 8)

    Returns:
        Reversed multivector, shape (..., 8)
    """
    r0 = mv[..., 0]
    r1 = mv[..., 1]
    r2 = mv[..., 2]
    r3 = mv[..., 3]
    r4 = -mv[..., 4]
    r5 = -mv[..., 5]
    r6 = -mv[..., 6]
    r7 = -mv[..., 7]

    return torch.stack([
        r0, r1, r2, r3, r4, r5, r6, r7,
    ], dim=-1)

# =============================================================================
# Sparse Operations (EvenVersor[4] x Point[3])
# =============================================================================

# EvenVersor sparse indices: (0, 4, 5, 6)
# Point sparse indices: (1, 2, 3)

@torch.jit.script
def upgc_encode(x: Tensor) -> Tensor:
    """
    Encode 1D vector to UPGC point representation.

    X = n_o + x + 0.5|x|^2 * n_inf

    Where:
        n_o = 0.5 * (e- - e+)   -> coefficients: e+ = -0.5, e- = 0.5
        n_inf = e- + e+         -> coefficients: e+ = 1, e- = 1

    Args:
        x: 1D vector, shape (..., 1)

    Returns:
        UPGC point, shape (..., 3)
    """
    x1 = x[..., 0]

    half_norm_sq = 0.5 * (x1 * x1)

    r0 = x1  # e1
    r1 = -0.5 + half_norm_sq  # e+
    r2 = 0.5 + half_norm_sq  # e-

    return torch.stack([r0, r1, r2], dim=-1)


@torch.jit.script
def upgc_decode(point: Tensor) -> Tensor:
    """
    Decode UPGC point to 1D vector.

    Extracts the first 1 components (euclidean part).

    Args:
        point: UPGC point, shape (..., 3)

    Returns:
        1D vector, shape (..., 1)
    """
    return point[..., :1]


@torch.jit.script
def reverse_even_versor(ev: Tensor) -> Tensor:
    """
    Compute reverse of an EvenVersor (sparse 4-component version).

    Args:
        ev: EvenVersor, shape (..., 4)

    Returns:
        Reversed EvenVersor, shape (..., 4)
    """
    r0 = ev[..., 0]  # 1: keep
    r1 = -ev[..., 1]  # e1e+: negate
    r2 = -ev[..., 2]  # e1e-: negate
    r3 = -ev[..., 3]  # e+e-: negate

    return torch.stack([r0, r1, r2, r3], dim=-1)


@torch.jit.script
def sandwich_product_sparse(ev: Tensor, point: Tensor) -> Tensor:
    """
    Compute sparse sandwich product: M × X × M̃

    Optimized for:
        - EvenVersor M: 4 components
        - Point X: 3 components
        - Output: 3 components

    Args:
        ev: EvenVersor, shape (..., 4)
        point: UPGC point, shape (..., 3)

    Returns:
        Transformed point, shape (..., 3)

    Note:
        Total multiplications: 72 (vs 96 for naive)
    """
    # EvenVersor components (sparse)
    m0 = ev[..., 0]  # 1
    m1 = ev[..., 1]  # e1e+
    m2 = ev[..., 2]  # e1e-
    m3 = ev[..., 3]  # e+e-

    # Point components (sparse)
    p0 = point[..., 0]  # e1
    p1 = point[..., 1]  # e+
    p2 = point[..., 2]  # e-

    # EvenVersor reverse
    mr0 = m0
    mr1 = -m1
    mr2 = -m2
    mr3 = -m3

    # Output r0 (e1): 12 terms
    r0 = (
        m0 * p0 * mr0 +
        -m0 * p1 * mr1 +
        m0 * p2 * mr2 +
        m1 * p0 * mr1 +
        m1 * p1 * mr0 +
        m1 * p2 * mr3 +
        -m2 * p0 * mr2 +
        -m2 * p1 * mr3 +
        -m2 * p2 * mr0 +
        m3 * p0 * mr3 +
        -m3 * p1 * mr2 +
        m3 * p2 * mr1
    )

    # Output r1 (e+): 12 terms
    r1 = (
        m0 * p0 * mr1 +
        m0 * p1 * mr0 +
        m0 * p2 * mr3 +
        -m1 * p0 * mr0 +
        m1 * p1 * mr1 +
        -m1 * p2 * mr2 +
        -m2 * p0 * mr3 +
        m2 * p1 * mr2 +
        -m2 * p2 * mr1 +
        -m3 * p0 * mr2 +
        -m3 * p1 * mr3 +
        -m3 * p2 * mr0
    )

    # Output r2 (e-): 12 terms
    r2 = (
        m0 * p0 * mr2 +
        m0 * p1 * mr3 +
        m0 * p2 * mr0 +
        -m1 * p0 * mr3 +
        m1 * p1 * mr2 +
        -m1 * p2 * mr1 +
        -m2 * p0 * mr0 +
        m2 * p1 * mr1 +
        -m2 * p2 * mr2 +
        -m3 * p0 * mr1 +
        -m3 * p1 * mr0 +
        -m3 * p2 * mr3
    )

    return torch.stack([r0, r1, r2], dim=-1)


# =============================================================================
# EvenVersor Composition
# =============================================================================

@torch.jit.script
def compose_even_versor(v1: Tensor, v2: Tensor) -> Tensor:
    """
    Compose two EvenVersors via geometric product.

    Args:
        v1: First EvenVersor, shape (..., 4)
        v2: Second EvenVersor, shape (..., 4)

    Returns:
        Composed EvenVersor, shape (..., 4)
    """
    # v1 components
    v1_0 = v1[..., 0]  # 1
    v1_1 = v1[..., 1]  # e1e+
    v1_2 = v1[..., 2]  # e1e-
    v1_3 = v1[..., 3]  # e+e-

    # v2 components
    v2_0 = v2[..., 0]  # 1
    v2_1 = v2[..., 1]  # e1e+
    v2_2 = v2[..., 2]  # e1e-
    v2_3 = v2[..., 3]  # e+e-

    # r0 (1): 4 terms
    r0 = (
        v1_0 * v2_0 +
        -v1_1 * v2_1 +
        v1_2 * v2_2 +
        v1_3 * v2_3
    )
    # r1 (e1e+): 4 terms
    r1 = (
        v1_0 * v2_1 +
        v1_1 * v2_0 +
        v1_2 * v2_3 +
        -v1_3 * v2_2
    )
    # r2 (e1e-): 4 terms
    r2 = (
        v1_0 * v2_2 +
        v1_1 * v2_3 +
        v1_2 * v2_0 +
        -v1_3 * v2_1
    )
    # r3 (e+e-): 4 terms
    r3 = (
        v1_0 * v2_3 +
        -v1_1 * v2_2 +
        v1_2 * v2_1 +
        v1_3 * v2_0
    )

    return torch.stack([r0, r1, r2, r3], dim=-1)

# =============================================================================
# Similitude Composition (CGA-specific accelerated)
# =============================================================================

@torch.jit.script
def compose_similitude(s1: Tensor, s2: Tensor) -> Tensor:
    """
    Compose two Similitudes via optimized geometric product.

    Similitude constraint: ei+ = ei- (translation components equal)
    This allows 30-50% speedup over general EvenVersor composition.

    Args:
        s1: First Similitude, shape (..., 4)
        s2: Second Similitude, shape (..., 4)

    Returns:
        Composed Similitude, shape (..., 4)
    """
    # Note: Currently same as compose_even_versor
    # Future optimization: exploit ei+ = ei- constraint
    # s1 components
    s1_0 = s1[..., 0]  # 1
    s1_1 = s1[..., 1]  # e1e+
    s1_2 = s1[..., 2]  # e1e-
    s1_3 = s1[..., 3]  # e+e-

    # s2 components
    s2_0 = s2[..., 0]  # 1
    s2_1 = s2[..., 1]  # e1e+
    s2_2 = s2[..., 2]  # e1e-
    s2_3 = s2[..., 3]  # e+e-

    # r0 (1): 4 terms
    r0 = (
        s1_0 * s2_0 +
        -s1_1 * s2_1 +
        s1_2 * s2_2 +
        s1_3 * s2_3
    )
    # r1 (e1e+): 4 terms
    r1 = (
        s1_0 * s2_1 +
        s1_1 * s2_0 +
        s1_2 * s2_3 +
        -s1_3 * s2_2
    )
    # r2 (e1e-): 4 terms
    r2 = (
        s1_0 * s2_2 +
        s1_1 * s2_3 +
        s1_2 * s2_0 +
        -s1_3 * s2_1
    )
    # r3 (e+e-): 4 terms
    r3 = (
        s1_0 * s2_3 +
        -s1_1 * s2_2 +
        s1_2 * s2_1 +
        s1_3 * s2_0
    )

    return torch.stack([r0, r1, r2, r3], dim=-1)

# =============================================================================
# Similitude Sandwich Product (CGA-specific accelerated)
# =============================================================================

@torch.jit.script
def sandwich_product_similitude(similitude: Tensor, point: Tensor) -> Tensor:
    """
    Compute sandwich product S × X × ~S for Similitude.

    Faster than sandwich_product_sparse by exploiting Similitude constraints.

    Args:
        similitude: Similitude, shape (..., 4)
        point: UPGC point, shape (..., 3)

    Returns:
        Transformed point, shape (..., 3)
    """
    # Currently delegates to sandwich_product_sparse
    # Future optimization: exploit ei+ = ei- constraint
    return sandwich_product_sparse(similitude, point)

# =============================================================================
# Geometric Inner Product (Metric Inner Product)
# =============================================================================

@torch.jit.script
def inner_product_full(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute geometric inner product with CGA metric.

    Args:
        a: First multivector, shape (..., 8)
        b: Second multivector, shape (..., 8)

    Returns:
        Scalar inner product, shape (..., 1)

    Note:
        Uses fused sign flipping: res = sum(a[i] * b[i] * METRIC_SIGNS[i])
    """
    r = (
        a[..., 0] * b[..., 0] +  # 1² = 1
        a[..., 1] * b[..., 1] +  # e1² = 1
        a[..., 2] * b[..., 2] +  # e+² = 1
        -a[..., 3] * b[..., 3] +  # e-² = -1
        -a[..., 4] * b[..., 4] +  # e1e+² = -1
        a[..., 5] * b[..., 5] +  # e1e-² = 1
        a[..., 6] * b[..., 6] +  # e+e-² = 1
        a[..., 7] * b[..., 7]  # e1e+e-² = 1
    )
    return r.unsqueeze(-1)

# =============================================================================
# Bivector Squared Scalar (Helper for exp_bivector)
# =============================================================================

@torch.jit.script
def bivector_squared_scalar(B: Tensor) -> Tensor:
    """
    Compute the scalar part of B².

    Args:
        B: Bivector, shape (..., 3)

    Returns:
        Scalar B² value, shape (...,)
    """
    r = (
        -B[..., 0] * B[..., 0] +
        B[..., 1] * B[..., 1] +
        B[..., 2] * B[..., 2]
    )
    return r

# =============================================================================
# Bivector Exponential Map
# =============================================================================

@torch.jit.script
def exp_bivector(B: Tensor) -> Tensor:
    """
    Compute exponential map from Bivector to EvenVersor.

    exp(B) = cos(θ) + sinc(θ) * B
    where θ² = -B² (for rotation bivectors)

    Args:
        B: Bivector, shape (..., 3)

    Returns:
        EvenVersor, shape (..., 4)

    Note:
        Uses torch.sinc for numerical stability at θ→0
    """
    # Compute B² scalar
    B_sq = bivector_squared_scalar(B)

    # θ² = -B² (for rotation bivectors, B² is negative)
    theta_sq = torch.clamp(-B_sq, min=1e-12)
    theta = torch.sqrt(theta_sq)

    # cos(θ) and sinc(θ) = sin(θ)/θ
    cos_theta = torch.cos(theta)
    # torch.sinc(x) = sin(πx)/(πx), so sinc(θ/π) = sin(θ)/θ
    sinc_theta = torch.sinc(theta / 3.141592653589793)

    # Build even_versor: scalar part = cos(θ), bivector parts = sinc(θ) * B
    r0 = cos_theta  # 1
    r1 = sinc_theta * B[..., 0]  # e1e+
    r2 = sinc_theta * B[..., 1]  # e1e-
    r3 = sinc_theta * B[..., 2]  # e+e-

    return torch.stack([r0, r1, r2, r3], dim=-1)

# =============================================================================
# Outer Product (Wedge Product)
# =============================================================================

@torch.jit.script
def outer_product_full(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute outer product (wedge product).

    a ∧ b = <a * b>_{grade(a) + grade(b)}

    Args:
        a: First multivector, shape (..., 8)
        b: Second multivector, shape (..., 8)

    Returns:
        Wedge product, shape (..., 8)
    """
    r0 = a[..., 0] * b[..., 0]  # 1
    r1 = a[..., 0] * b[..., 1] + a[..., 1] * b[..., 0]  # e1
    r2 = a[..., 0] * b[..., 2] + a[..., 2] * b[..., 0]  # e+
    r3 = a[..., 0] * b[..., 3] + a[..., 3] * b[..., 0]  # e-
    r4 = (  # e1e+
        a[..., 0] * b[..., 4] +
        a[..., 1] * b[..., 2] +
        -a[..., 2] * b[..., 1] +
        a[..., 4] * b[..., 0]
    )
    r5 = (  # e1e-
        a[..., 0] * b[..., 5] +
        a[..., 1] * b[..., 3] +
        -a[..., 3] * b[..., 1] +
        a[..., 5] * b[..., 0]
    )
    r6 = (  # e+e-
        a[..., 0] * b[..., 6] +
        a[..., 2] * b[..., 3] +
        -a[..., 3] * b[..., 2] +
        a[..., 6] * b[..., 0]
    )
    r7 = (  # e1e+e-
        a[..., 0] * b[..., 7] +
        a[..., 1] * b[..., 6] +
        -a[..., 2] * b[..., 5] +
        a[..., 3] * b[..., 4] +
        a[..., 4] * b[..., 3] +
        -a[..., 5] * b[..., 2] +
        a[..., 6] * b[..., 1] +
        a[..., 7] * b[..., 0]
    )

    return torch.stack([
        r0, r1, r2, r3, r4, r5, r6, r7,
    ], dim=-1)

# =============================================================================
# Left Contraction
# =============================================================================

@torch.jit.script
def left_contraction_full(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute left contraction.

    a ⌋ b = <a * b>_{grade(b) - grade(a)} if grade(a) <= grade(b), else 0

    Args:
        a: First multivector, shape (..., 8)
        b: Second multivector, shape (..., 8)

    Returns:
        Left contraction result, shape (..., 8)
    """
    r0 = (  # 1
        a[..., 0] * b[..., 0] +
        a[..., 1] * b[..., 1] +
        a[..., 2] * b[..., 2] +
        -a[..., 3] * b[..., 3] +
        -a[..., 4] * b[..., 4] +
        a[..., 5] * b[..., 5] +
        a[..., 6] * b[..., 6] +
        a[..., 7] * b[..., 7]
    )
    r1 = (  # e1
        a[..., 0] * b[..., 1] +
        -a[..., 2] * b[..., 4] +
        a[..., 3] * b[..., 5] +
        a[..., 6] * b[..., 7]
    )
    r2 = (  # e+
        a[..., 0] * b[..., 2] +
        a[..., 1] * b[..., 4] +
        a[..., 3] * b[..., 6] +
        -a[..., 5] * b[..., 7]
    )
    r3 = (  # e-
        a[..., 0] * b[..., 3] +
        a[..., 1] * b[..., 5] +
        a[..., 2] * b[..., 6] +
        -a[..., 4] * b[..., 7]
    )
    r4 = a[..., 0] * b[..., 4] + -a[..., 3] * b[..., 7]  # e1e+
    r5 = a[..., 0] * b[..., 5] + -a[..., 2] * b[..., 7]  # e1e-
    r6 = a[..., 0] * b[..., 6] + a[..., 1] * b[..., 7]  # e+e-
    r7 = a[..., 0] * b[..., 7]  # e1e+e-

    return torch.stack([
        r0, r1, r2, r3, r4, r5, r6, r7,
    ], dim=-1)

# =============================================================================
# Right Contraction
# =============================================================================

@torch.jit.script
def right_contraction_full(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute right contraction.

    a ⌊ b = <a * b>_{grade(a) - grade(b)} if grade(a) >= grade(b), else 0

    Args:
        a: First multivector, shape (..., 8)
        b: Second multivector, shape (..., 8)

    Returns:
        Right contraction result, shape (..., 8)
    """
    r0 = (  # 1
        a[..., 0] * b[..., 0] +
        a[..., 1] * b[..., 1] +
        a[..., 2] * b[..., 2] +
        -a[..., 3] * b[..., 3] +
        -a[..., 4] * b[..., 4] +
        a[..., 5] * b[..., 5] +
        a[..., 6] * b[..., 6] +
        a[..., 7] * b[..., 7]
    )
    r1 = (  # e1
        a[..., 1] * b[..., 0] +
        a[..., 4] * b[..., 2] +
        -a[..., 5] * b[..., 3] +
        a[..., 7] * b[..., 6]
    )
    r2 = (  # e+
        a[..., 2] * b[..., 0] +
        -a[..., 4] * b[..., 1] +
        -a[..., 6] * b[..., 3] +
        -a[..., 7] * b[..., 5]
    )
    r3 = (  # e-
        a[..., 3] * b[..., 0] +
        -a[..., 5] * b[..., 1] +
        -a[..., 6] * b[..., 2] +
        -a[..., 7] * b[..., 4]
    )
    r4 = a[..., 4] * b[..., 0] + -a[..., 7] * b[..., 3]  # e1e+
    r5 = a[..., 5] * b[..., 0] + -a[..., 7] * b[..., 2]  # e1e-
    r6 = a[..., 6] * b[..., 0] + a[..., 7] * b[..., 1]  # e+e-
    r7 = a[..., 7] * b[..., 0]  # e1e+e-

    return torch.stack([
        r0, r1, r2, r3, r4, r5, r6, r7,
    ], dim=-1)

# =============================================================================
# Grade Selection
# =============================================================================

# Grade masks for grade selection
GRADE_0_MASK = (0,)
GRADE_1_MASK = (1, 2, 3)
GRADE_2_MASK = (4, 5, 6)
GRADE_3_MASK = (7,)

@torch.jit.script
def grade_select(mv: Tensor, grade: int) -> Tensor:
    """
    Extract components of a specific grade.

    Args:
        mv: Multivector, shape (..., 8)
        grade: Grade to extract (0 to 3)

    Returns:
        Grade components (zeros for non-selected grades)
    """
    if grade == 0:
        r0 = mv[..., 0]
        r1 = torch.zeros_like(mv[..., 0])
        r2 = torch.zeros_like(mv[..., 0])
        r3 = torch.zeros_like(mv[..., 0])
        r4 = torch.zeros_like(mv[..., 0])
        r5 = torch.zeros_like(mv[..., 0])
        r6 = torch.zeros_like(mv[..., 0])
        r7 = torch.zeros_like(mv[..., 0])
        return torch.stack([
            r0, r1, r2, r3, r4, r5, r6, r7,
        ], dim=-1)
    elif grade == 1:
        r0 = torch.zeros_like(mv[..., 0])
        r1 = mv[..., 1]
        r2 = mv[..., 2]
        r3 = mv[..., 3]
        r4 = torch.zeros_like(mv[..., 0])
        r5 = torch.zeros_like(mv[..., 0])
        r6 = torch.zeros_like(mv[..., 0])
        r7 = torch.zeros_like(mv[..., 0])
        return torch.stack([
            r0, r1, r2, r3, r4, r5, r6, r7,
        ], dim=-1)
    elif grade == 2:
        r0 = torch.zeros_like(mv[..., 0])
        r1 = torch.zeros_like(mv[..., 0])
        r2 = torch.zeros_like(mv[..., 0])
        r3 = torch.zeros_like(mv[..., 0])
        r4 = mv[..., 4]
        r5 = mv[..., 5]
        r6 = mv[..., 6]
        r7 = torch.zeros_like(mv[..., 0])
        return torch.stack([
            r0, r1, r2, r3, r4, r5, r6, r7,
        ], dim=-1)
    elif grade == 3:
        r0 = torch.zeros_like(mv[..., 0])
        r1 = torch.zeros_like(mv[..., 0])
        r2 = torch.zeros_like(mv[..., 0])
        r3 = torch.zeros_like(mv[..., 0])
        r4 = torch.zeros_like(mv[..., 0])
        r5 = torch.zeros_like(mv[..., 0])
        r6 = torch.zeros_like(mv[..., 0])
        r7 = mv[..., 7]
        return torch.stack([
            r0, r1, r2, r3, r4, r5, r6, r7,
        ], dim=-1)
    else:
        return torch.zeros_like(mv)

# =============================================================================
# Dual Operation
# =============================================================================

# Pseudoscalar index: 7, I² = 1
# I^{-1} = I / I² = I * 1.0

@torch.jit.script
def dual(mv: Tensor) -> Tensor:
    """
    Compute the dual of a multivector.

    a* = a ⌋ I^{-1} (left contraction with pseudoscalar inverse)

    Args:
        mv: Multivector, shape (..., 8)

    Returns:
        Dual multivector, shape (..., 8)
    """
    r0 = mv[..., 7]  # 1
    r1 = mv[..., 6]  # e1
    r2 = -mv[..., 5]  # e+
    r3 = -mv[..., 4]  # e-
    r4 = -mv[..., 3]  # e1e+
    r5 = -mv[..., 2]  # e1e-
    r6 = mv[..., 1]  # e+e-
    r7 = mv[..., 0]  # e1e+e-

    return torch.stack([
        r0, r1, r2, r3, r4, r5, r6, r7,
    ], dim=-1)

# =============================================================================
# Normalize Operation
# =============================================================================

@torch.jit.script
def norm_squared(mv: Tensor) -> Tensor:
    """
    Compute squared norm of a multivector.

    |a|² = <a * ~a>_0

    Args:
        mv: Multivector, shape (..., 8)

    Returns:
        Squared norm, shape (...,)
    """
    r = (
        mv[..., 0] * mv[..., 0] +
        mv[..., 1] * mv[..., 1] +
        mv[..., 2] * mv[..., 2] +
        -mv[..., 3] * mv[..., 3] +
        mv[..., 4] * mv[..., 4] +
        -mv[..., 5] * mv[..., 5] +
        -mv[..., 6] * mv[..., 6] +
        -mv[..., 7] * mv[..., 7]
    )
    return r


@torch.jit.script
def normalize(mv: Tensor) -> Tensor:
    """
    Normalize a multivector to unit norm.

    Args:
        mv: Multivector, shape (..., 8)

    Returns:
        Normalized multivector, shape (..., 8)

    Note:
        Returns NaN for null multivectors (ONNX compatible)
    """
    norm = torch.sqrt(torch.abs(norm_squared(mv)) + 1e-12)
    return mv / norm.unsqueeze(-1)

# =============================================================================
# Structure Normalize (US9a)
# =============================================================================

ROTOR_INDICES: Tuple[int, ...] = (0,)
TRANSLATION_PAIRS: Tuple[Tuple[int, int], ...] = ((1, 2),)
DILATION_INDEX: int = 3


@torch.jit.script
def structure_normalize(similitude: Tensor, eps: float = 1e-8) -> Tensor:
    """
    Structure-normalize a Similitude tensor.

    Ensures:
    1. Rotor part (scalar + spatial bivectors) has unit norm
    2. Similitude constraint: eie+ = eie- (no transversion)

    Args:
        similitude: Similitude tensor, shape (..., 4)
        eps: Numerical stability constant

    Returns:
        Structure-normalized Similitude, shape (..., 4)
    """
    result = similitude.clone()

    # Step 1: Normalize Rotor part
    rotor_norm_sq = similitude[..., 0] * similitude[..., 0]
    rotor_norm = torch.sqrt(rotor_norm_sq + eps)
    result[..., 0] = similitude[..., 0] / rotor_norm

    # Step 2: Enforce Similitude constraint (eie+ = eie-)
    avg_1 = (result[..., 1] + result[..., 2]) / 2
    result[..., 1] = avg_1
    result[..., 2] = avg_1

    return result


@torch.jit.script
def soft_structure_normalize(similitude: Tensor, strength: float = 1.0, eps: float = 1e-8) -> Tensor:
    """
    Soft structure normalization with interpolation.

    Args:
        similitude: Similitude tensor, shape (..., 4)
        strength: Interpolation strength (0=no change, 1=full normalize)
        eps: Numerical stability constant

    Returns:
        Soft-normalized Similitude, shape (..., 4)
    """
    normalized = structure_normalize(similitude, eps)
    return similitude + strength * (normalized - similitude)
