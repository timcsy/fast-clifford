"""
CGA1D Cl(2,1) Functional Operations - Auto-generated

DO NOT EDIT MANUALLY - This file is generated by codegen/generate.py

Generated: 2025-12-07 23:09:37
Algebra: cga1d
Signature: (1, 1, -1)
Blade count: 8

All functions are:
- Loop-free (fully expanded arithmetic)
- ONNX-compatible (only Add/Mul/Neg/Sub operations)
- Hard-coded (no Cayley table lookups)
"""

import torch
from torch import Tensor
from typing import Tuple


# =============================================================================
# Constants
# =============================================================================

BLADE_COUNT = 8
EUCLIDEAN_DIM = 1

# Blade indices by grade
GRADE_0_INDICES = (0,)
GRADE_1_INDICES = (1, 2, 3)
GRADE_2_INDICES = (4, 5, 6)
GRADE_3_INDICES = (7,)

# Sparsity masks
UPGC_POINT_MASK = (1, 2, 3)  # 3 components
MOTOR_MASK = (0, 4, 5, 6)  # 4 components

# Reverse signs for all 8 blades
REVERSE_SIGNS = (1, 1, 1, 1, -1, -1, -1, -1)

# Motor-specific reverse signs (4 components)
MOTOR_REVERSE_SIGNS = (1, -1, -1, -1)

# =============================================================================
# Geometric Product (Full 8x8)
# =============================================================================

@torch.jit.script
def geometric_product_full(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute the full geometric product of two multivectors.

    Args:
        a: Left operand, shape (..., 8)
        b: Right operand, shape (..., 8)

    Returns:
        Result multivector, shape (..., 8)

    Note:
        Fully expanded, no loops, ONNX compatible.
    """
    r0 = (
        a[..., 0] * b[..., 0] +
        a[..., 1] * b[..., 1] +
        a[..., 2] * b[..., 2] +
        -a[..., 3] * b[..., 3] +
        -a[..., 4] * b[..., 4] +
        a[..., 5] * b[..., 5] +
        a[..., 6] * b[..., 6] +
        a[..., 7] * b[..., 7]
    )
    r1 = (
        a[..., 0] * b[..., 1] +
        a[..., 1] * b[..., 0] +
        -a[..., 2] * b[..., 4] +
        a[..., 3] * b[..., 5] +
        a[..., 4] * b[..., 2] +
        -a[..., 5] * b[..., 3] +
        a[..., 6] * b[..., 7] +
        a[..., 7] * b[..., 6]
    )
    r2 = (
        a[..., 0] * b[..., 2] +
        a[..., 1] * b[..., 4] +
        a[..., 2] * b[..., 0] +
        a[..., 3] * b[..., 6] +
        -a[..., 4] * b[..., 1] +
        -a[..., 5] * b[..., 7] +
        -a[..., 6] * b[..., 3] +
        -a[..., 7] * b[..., 5]
    )
    r3 = (
        a[..., 0] * b[..., 3] +
        a[..., 1] * b[..., 5] +
        a[..., 2] * b[..., 6] +
        a[..., 3] * b[..., 0] +
        -a[..., 4] * b[..., 7] +
        -a[..., 5] * b[..., 1] +
        -a[..., 6] * b[..., 2] +
        -a[..., 7] * b[..., 4]
    )
    r4 = (
        a[..., 0] * b[..., 4] +
        a[..., 1] * b[..., 2] +
        -a[..., 2] * b[..., 1] +
        -a[..., 3] * b[..., 7] +
        a[..., 4] * b[..., 0] +
        a[..., 5] * b[..., 6] +
        -a[..., 6] * b[..., 5] +
        -a[..., 7] * b[..., 3]
    )
    r5 = (
        a[..., 0] * b[..., 5] +
        a[..., 1] * b[..., 3] +
        -a[..., 2] * b[..., 7] +
        -a[..., 3] * b[..., 1] +
        a[..., 4] * b[..., 6] +
        a[..., 5] * b[..., 0] +
        -a[..., 6] * b[..., 4] +
        -a[..., 7] * b[..., 2]
    )
    r6 = (
        a[..., 0] * b[..., 6] +
        a[..., 1] * b[..., 7] +
        a[..., 2] * b[..., 3] +
        -a[..., 3] * b[..., 2] +
        -a[..., 4] * b[..., 5] +
        a[..., 5] * b[..., 4] +
        a[..., 6] * b[..., 0] +
        a[..., 7] * b[..., 1]
    )
    r7 = (
        a[..., 0] * b[..., 7] +
        a[..., 1] * b[..., 6] +
        -a[..., 2] * b[..., 5] +
        a[..., 3] * b[..., 4] +
        a[..., 4] * b[..., 3] +
        -a[..., 5] * b[..., 2] +
        a[..., 6] * b[..., 1] +
        a[..., 7] * b[..., 0]
    )

    return torch.stack([
        r0, r1, r2, r3, r4, r5, r6, r7,
    ], dim=-1)

# =============================================================================
# Reverse Operation
# =============================================================================

@torch.jit.script
def reverse_full(mv: Tensor) -> Tensor:
    """
    Compute the reverse of a multivector.

    For grade k: coefficient *= (-1)^(k*(k-1)/2)

    Args:
        mv: Input multivector, shape (..., 8)

    Returns:
        Reversed multivector, shape (..., 8)
    """
    r0 = mv[..., 0]
    r1 = mv[..., 1]
    r2 = mv[..., 2]
    r3 = mv[..., 3]
    r4 = -mv[..., 4]
    r5 = -mv[..., 5]
    r6 = -mv[..., 6]
    r7 = -mv[..., 7]

    return torch.stack([
        r0, r1, r2, r3, r4, r5, r6, r7,
    ], dim=-1)

# =============================================================================
# Sparse Operations (Motor[4] x Point[3])
# =============================================================================

# Motor sparse indices: (0, 4, 5, 6)
# Point sparse indices: (1, 2, 3)

@torch.jit.script
def upgc_encode(x: Tensor) -> Tensor:
    """
    Encode 1D vector to UPGC point representation.

    X = n_o + x + 0.5|x|^2 * n_inf

    Where:
        n_o = 0.5 * (e- - e+)   -> coefficients: e+ = -0.5, e- = 0.5
        n_inf = e- + e+         -> coefficients: e+ = 1, e- = 1

    Args:
        x: 1D vector, shape (..., 1)

    Returns:
        UPGC point, shape (..., 3)
    """
    x1 = x[..., 0]

    half_norm_sq = 0.5 * (x1 * x1)

    r0 = x1  # e1
    r1 = -0.5 + half_norm_sq  # e+
    r2 = 0.5 + half_norm_sq  # e-

    return torch.stack([r0, r1, r2], dim=-1)


@torch.jit.script
def upgc_decode(point: Tensor) -> Tensor:
    """
    Decode UPGC point to 1D vector.

    Extracts the first 1 components (euclidean part).

    Args:
        point: UPGC point, shape (..., 3)

    Returns:
        1D vector, shape (..., 1)
    """
    return point[..., :1]


@torch.jit.script
def reverse_motor(motor: Tensor) -> Tensor:
    """
    Compute reverse of a motor (sparse 4-component version).

    Args:
        motor: Motor, shape (..., 4)

    Returns:
        Reversed motor, shape (..., 4)
    """
    r0 = motor[..., 0]  # 1: keep
    r1 = -motor[..., 1]  # e1e+: negate
    r2 = -motor[..., 2]  # e1e-: negate
    r3 = -motor[..., 3]  # e+e-: negate

    return torch.stack([r0, r1, r2, r3], dim=-1)


@torch.jit.script
def sandwich_product_sparse(motor: Tensor, point: Tensor) -> Tensor:
    """
    Compute sparse sandwich product: M × X × M̃

    Optimized for:
        - Motor M: 4 components
        - Point X: 3 components
        - Output: 3 components

    Args:
        motor: Motor, shape (..., 4)
        point: UPGC point, shape (..., 3)

    Returns:
        Transformed point, shape (..., 3)

    Note:
        Total multiplications: 72 (vs 96 for naive)
    """
    # Motor components (sparse)
    m0 = motor[..., 0]  # 1
    m1 = motor[..., 1]  # e1e+
    m2 = motor[..., 2]  # e1e-
    m3 = motor[..., 3]  # e+e-

    # Point components (sparse)
    p0 = point[..., 0]  # e1
    p1 = point[..., 1]  # e+
    p2 = point[..., 2]  # e-

    # Motor reverse
    mr0 = m0
    mr1 = -m1
    mr2 = -m2
    mr3 = -m3

    # Output r0 (e1): 12 terms
    r0 = (
        m0 * p0 * mr0 +
        -m0 * p1 * mr1 +
        m0 * p2 * mr2 +
        m1 * p0 * mr1 +
        m1 * p1 * mr0 +
        m1 * p2 * mr3 +
        -m2 * p0 * mr2 +
        -m2 * p1 * mr3 +
        -m2 * p2 * mr0 +
        m3 * p0 * mr3 +
        -m3 * p1 * mr2 +
        m3 * p2 * mr1
    )

    # Output r1 (e+): 12 terms
    r1 = (
        m0 * p0 * mr1 +
        m0 * p1 * mr0 +
        m0 * p2 * mr3 +
        -m1 * p0 * mr0 +
        m1 * p1 * mr1 +
        -m1 * p2 * mr2 +
        -m2 * p0 * mr3 +
        m2 * p1 * mr2 +
        -m2 * p2 * mr1 +
        -m3 * p0 * mr2 +
        -m3 * p1 * mr3 +
        -m3 * p2 * mr0
    )

    # Output r2 (e-): 12 terms
    r2 = (
        m0 * p0 * mr2 +
        m0 * p1 * mr3 +
        m0 * p2 * mr0 +
        -m1 * p0 * mr3 +
        m1 * p1 * mr2 +
        -m1 * p2 * mr1 +
        -m2 * p0 * mr0 +
        m2 * p1 * mr1 +
        -m2 * p2 * mr2 +
        -m3 * p0 * mr1 +
        -m3 * p1 * mr0 +
        -m3 * p2 * mr3
    )

    return torch.stack([r0, r1, r2], dim=-1)

