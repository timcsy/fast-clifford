"""
Clifford Algebra Cl(0, 0) Functional Operations - Auto-generated

DO NOT EDIT MANUALLY - This file is generated by codegen/generator.py

Generated: 2025-12-18 23:44:38
Signature: Cl(0, 0)
Blade count: 1
Rotor count: 1
Bivector count: 0
Algebra type: vga

All functions are:
- Loop-free (fully expanded arithmetic)
- ONNX-compatible (only Add/Mul/Neg/Sub operations)
- torch.jit.script compatible
"""

import torch
from torch import Tensor
from typing import Tuple


# =============================================================================
# Constants
# =============================================================================

BLADE_COUNT = 1
ROTOR_COUNT = 1
BIVECTOR_COUNT = 0

# Blade indices by grade
GRADE_0_INDICES = (0,)

# Rotor (even-grade) indices
ROTOR_MASK = (0,)

# Bivector (grade-2) indices
BIVECTOR_MASK = ()

# Vector (grade-1) indices
VECTOR_MASK = ()

# Reverse signs for all blades
REVERSE_SIGNS = (1,)

# Involute signs for all blades
INVOLUTE_SIGNS = (1,)

# Conjugate signs for all blades
CONJUGATE_SIGNS = (1,)

# Inner product signs (blade² values)
INNER_PRODUCT_SIGNS = (1,)

# Pseudoscalar info
PSEUDOSCALAR_INDEX = 0
PSEUDOSCALAR_SQUARE = 1

# =============================================================================
# Geometric Product
# =============================================================================

@torch.jit.script
def geometric_product(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute geometric product of two multivectors in Cl(0, 0).

    Args:
        a: Left operand, shape (..., 1)
        b: Right operand, shape (..., 1)

    Returns:
        Result multivector, shape (..., 1)
    """
    r0 = a[..., 0] * b[..., 0]

    return torch.stack([
        r0,
    ], dim=-1)

# =============================================================================
# Reverse Operation
# =============================================================================

@torch.jit.script
def reverse(mv: Tensor) -> Tensor:
    """
    Compute reverse of a multivector in Cl(0, 0).

    For grade k: coefficient *= (-1)^(k*(k-1)/2)

    Args:
        mv: Input multivector, shape (..., 1)

    Returns:
        Reversed multivector, shape (..., 1)
    """
    r0 = mv[..., 0]

    return torch.stack([
        r0,
    ], dim=-1)

# =============================================================================
# Grade Involution
# =============================================================================

@torch.jit.script
def involute(mv: Tensor) -> Tensor:
    """
    Compute grade involution of a multivector in Cl(0, 0).

    For grade k: coefficient *= (-1)^k

    Args:
        mv: Input multivector, shape (..., 1)

    Returns:
        Grade-involuted multivector, shape (..., 1)
    """
    r0 = mv[..., 0]

    return torch.stack([
        r0,
    ], dim=-1)

# =============================================================================
# Clifford Conjugate
# =============================================================================

@torch.jit.script
def conjugate(mv: Tensor) -> Tensor:
    """
    Compute Clifford conjugate of a multivector in Cl(0, 0).

    For grade k: coefficient *= (-1)^(k*(k+1)/2)

    Args:
        mv: Input multivector, shape (..., 1)

    Returns:
        Conjugated multivector, shape (..., 1)
    """
    r0 = mv[..., 0]

    return torch.stack([
        r0,
    ], dim=-1)

# =============================================================================
# Grade Selection
# =============================================================================

@torch.jit.script
def select_grade_0(mv: Tensor) -> Tensor:
    """
    Extract grade-0 components from multivector.

    Args:
        mv: Input multivector, shape (..., 1)

    Returns:
        Multivector with only grade-0 components, shape (..., 1)
    """
    r0 = mv[..., 0]

    return torch.stack([
        r0,
    ], dim=-1)


# =============================================================================
# Inner Product
# =============================================================================

@torch.jit.script
def inner(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute inner product <ab>_0 in Cl(0, 0).

    Args:
        a: Left operand, shape (..., 1)
        b: Right operand, shape (..., 1)

    Returns:
        Scalar result, shape (..., 1)
    """
    result = a[..., 0] * b[..., 0]
    return result.unsqueeze(-1)

# =============================================================================
# Outer Product (Wedge)
# =============================================================================

@torch.jit.script
def outer(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute outer product a ^ b in Cl(0, 0).

    Args:
        a: Left operand, shape (..., 1)
        b: Right operand, shape (..., 1)

    Returns:
        Result multivector, shape (..., 1)
    """
    r0 = a[..., 0] * b[..., 0]

    return torch.stack([
        r0,
    ], dim=-1)

# =============================================================================
# Left Contraction
# =============================================================================

@torch.jit.script
def contract_left(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute left contraction a << b in Cl(0, 0).

    Args:
        a: Left operand, shape (..., 1)
        b: Right operand, shape (..., 1)

    Returns:
        Result multivector, shape (..., 1)
    """
    r0 = a[..., 0] * b[..., 0]

    return torch.stack([
        r0,
    ], dim=-1)

# =============================================================================
# Right Contraction
# =============================================================================

@torch.jit.script
def contract_right(a: Tensor, b: Tensor) -> Tensor:
    """
    Compute right contraction a >> b in Cl(0, 0).

    Args:
        a: Left operand, shape (..., 1)
        b: Right operand, shape (..., 1)

    Returns:
        Result multivector, shape (..., 1)
    """
    r0 = a[..., 0] * b[..., 0]

    return torch.stack([
        r0,
    ], dim=-1)

# =============================================================================
# Dual (Left Contraction with Pseudoscalar)
# =============================================================================

@torch.jit.script
def dual(mv: Tensor) -> Tensor:
    """
    Compute dual mv* = mv << I in Cl(0, 0).

    Args:
        mv: Input multivector, shape (..., 1)

    Returns:
        Dual multivector, shape (..., 1)
    """
    r0 = mv[..., 0]

    return torch.stack([
        r0,
    ], dim=-1)

# =============================================================================
# Norm Squared
# =============================================================================

@torch.jit.script
def norm_squared(mv: Tensor) -> Tensor:
    """
    Compute norm squared |mv|^2 = <mv * ~mv>_0 in Cl(0, 0).

    Args:
        mv: Input multivector, shape (..., 1)

    Returns:
        Norm squared, shape (..., 1)
    """
    result = mv[..., 0] * mv[..., 0]
    return result.unsqueeze(-1)

# =============================================================================
# Rotor Composition
# =============================================================================

@torch.jit.script
def compose_rotor(r1: Tensor, r2: Tensor) -> Tensor:
    """
    Compose two rotors r1 * r2 in Cl(0, 0).

    Args:
        r1: Left rotor, shape (..., 1)
        r2: Right rotor, shape (..., 1)

    Returns:
        Composed rotor, shape (..., 1)
    """
    out0 = r1[..., 0] * r2[..., 0]

    return torch.stack([
        out0,
    ], dim=-1)

# =============================================================================
# Rotor Reverse
# =============================================================================

@torch.jit.script
def reverse_rotor(r: Tensor) -> Tensor:
    """
    Compute reverse of a rotor in Cl(0, 0).

    Args:
        r: Input rotor, shape (..., 1)

    Returns:
        Reversed rotor, shape (..., 1)
    """
    r0 = r[..., 0]

    return torch.stack([
        r0,
    ], dim=-1)

# =============================================================================
# Rotor Sandwich Product
# =============================================================================

@torch.jit.script
def sandwich_rotor(r: Tensor, x: Tensor) -> Tensor:
    """
    Compute sandwich product r * x * ~r in Cl(0, 0).

    Args:
        r: Rotor, shape (..., 1)
        x: Operand multivector, shape (..., 1)

    Returns:
        Transformed multivector, shape (..., 1)

    Note:
        This is a simplified implementation. Optimized sparse versions
        can be generated for specific input types (e.g., vectors).
    """
    # Expand rotor to full multivector
    rf0 = r[..., 0]

    # Compute r * x
    t0 = rf0 * x[..., 0]

    # Compute ~r
    rr0 = rf0

    # Compute (r * x) * ~r
    s0 = t0 * rr0

    return torch.stack([
        s0,
    ], dim=-1)

# =============================================================================
# Rotor Norm Squared
# =============================================================================

@torch.jit.script
def norm_squared_rotor(r: Tensor) -> Tensor:
    """
    Compute norm squared of a rotor in Cl(0, 0).

    Args:
        r: Input rotor, shape (..., 1)

    Returns:
        Norm squared, shape (..., 1)
    """
    result = r[..., 0] * r[..., 0]
    return result.unsqueeze(-1)

# =============================================================================
# Bivector Exponential (exp_bivector)
# =============================================================================

@torch.jit.script
def exp_bivector(B: Tensor) -> Tensor:
    """
    Compute exponential of a bivector in Cl(0, 0).

    Converts a bivector (generator) to a rotor (rotation).
    Formula: exp(B) = cos(θ) + sin(θ)/θ * B where θ = sqrt(-B²)

    Args:
        B: Input bivector, shape (..., 0)

    Returns:
        Rotor, shape (..., 1)
    """
    B_sq = torch.zeros_like(B[..., 0])

    # θ² = -B² (bivectors square to negative in Euclidean)
    theta_sq = -B_sq

    # Handle both positive (elliptic) and negative (hyperbolic) cases
    # For numerical stability, use small angle approximation when θ ≈ 0
    eps = 1e-8

    # Elliptic case: θ² > 0 => exp(B) = cos(θ) + sin(θ)/θ * B
    # Hyperbolic case: θ² < 0 => exp(B) = cosh(θ) + sinh(θ)/θ * B
    theta = torch.sqrt(torch.abs(theta_sq) + eps)

    # Compute cos/cosh and sinc (sin(θ)/θ or sinh(θ)/θ)
    is_elliptic = theta_sq > 0
    cos_term = torch.where(is_elliptic, torch.cos(theta), torch.cosh(theta))
    sinc_term = torch.where(
        is_elliptic,
        torch.sin(theta) / theta,
        torch.sinh(theta) / theta
    )

    # Build rotor: scalar part + bivector parts
    r0 = cos_term

    return torch.stack([
        r0,
    ], dim=-1)

# =============================================================================
# Rotor Logarithm (log_rotor)
# =============================================================================

@torch.jit.script
def log_rotor(r: Tensor) -> Tensor:
    """
    Compute logarithm of a rotor in Cl(0, 0).

    Converts a rotor (rotation) to a bivector (generator).
    Formula: log(R) = atan2(|B|, s) / |B| * B where R = s + B

    Args:
        r: Input rotor, shape (..., 1)

    Returns:
        Bivector, shape (..., 0)
    """
    s = r[..., 0]  # Scalar part

    # Compute |B|² from bivector parts of rotor
    B_norm_sq = torch.zeros_like(s)

    # Compute angle θ = atan2(|B|, s)
    eps = 1e-8
    B_norm = torch.sqrt(B_norm_sq + eps)
    theta = torch.atan2(B_norm, s)

    # Scale factor: θ / |B|
    scale = theta / (B_norm + eps)

    # Extract bivector components from rotor

    return torch.stack([
    ], dim=-1)

# =============================================================================
# Rotor SLERP (Spherical Linear Interpolation)
# =============================================================================

@torch.jit.script
def slerp_rotor(r1: Tensor, r2: Tensor, t: Tensor) -> Tensor:
    """
    Spherical linear interpolation between rotors in Cl(0, 0).

    Formula: slerp(r1, r2, t) = r1 * exp(t * log(r1~ * r2))

    Args:
        r1: Start rotor, shape (..., 1)
        r2: End rotor, shape (..., 1)
        t: Interpolation parameter [0, 1], shape (...) or scalar

    Returns:
        Interpolated rotor, shape (..., 1)
    """
    # Compute r1~ (reverse of r1)
    r1_rev = reverse_rotor(r1)

    # Compute delta = r1~ * r2
    delta = compose_rotor(r1_rev, r2)

    # Compute log of delta to get bivector
    log_delta = log_rotor(delta)

    # Scale by t
    if t.dim() == 0:
        scaled_log = t * log_delta
    else:
        scaled_log = t.unsqueeze(-1) * log_delta

    # Exponentiate back to rotor
    delta_t = exp_bivector(scaled_log)

    # Compose with r1
    return compose_rotor(r1, delta_t)
