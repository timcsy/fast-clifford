"""
Code generator for CGA Clifford algebra operations.

Generates hard-coded PyTorch functions with no loops,
fully expanded arithmetic for ONNX compatibility.

Supports multiple CGA dimensions:
- CGA1D Cl(2,1): 8 blades
- CGA2D Cl(3,1): 16 blades
- CGA3D Cl(4,1): 32 blades
"""

from typing import Dict, List, Tuple, Optional
from datetime import datetime

from .base import AlgebraDefinition, CodeGenerator, SparsityPattern
from .cga_factory import (
    create_cga_algebra,
    compute_blade_count,
    compute_grade_indices,
    compute_reverse_signs,
    get_product_table,
    get_upgc_point_indices,
    get_even_versor_indices,
    get_blade_names,
)
from .sparse_analysis import (
    get_compose_even_versor_terms,
    get_compose_similitude_terms,
    get_inner_product_signs,
    get_bivector_squared_terms,
    get_bivector_indices,
    get_outer_product_terms,
    get_left_contraction_terms,
    get_right_contraction_terms,
    get_grade_masks,
    get_pseudoscalar_info,
    get_norm_squared_terms,
    get_rotor_indices,
    get_translation_pairs,
    get_dilation_index,
)


class CGA3DAlgebra(AlgebraDefinition):
    """
    CGA Cl(4,1) algebra definition for code generation.

    Wraps the algebra.py module for use with the generator.
    """

    def __init__(self):
        # Import here to avoid circular imports
        from fast_clifford.algebras.cga3d import algebra

        self._algebra = algebra
        self._product_table = algebra.get_product_table()
        self._reverse_signs = algebra.REVERSE_SIGNS

    @property
    def name(self) -> str:
        return "cga3d"

    @property
    def signature(self) -> Tuple[int, ...]:
        return (1, 1, 1, 1, -1)

    @property
    def blade_count(self) -> int:
        return 32

    def get_grade_indices(self, grade: int) -> Tuple[int, ...]:
        return self._algebra.GRADE_INDICES[grade]

    def get_product_table(self) -> Dict[Tuple[int, int], Tuple[int, int]]:
        return self._product_table

    def get_reverse_signs(self) -> Tuple[int, ...]:
        return self._reverse_signs


class CGA3DCodeGenerator(CodeGenerator):
    """
    Code generator for CGA Cl(4,1) operations.

    Generates PyTorch code with:
    - Hard-coded arithmetic (no Cayley table lookups)
    - No loops (fully expanded)
    - ONNX-compatible operations only
    """

    def __init__(self, algebra: Optional[AlgebraDefinition] = None):
        if algebra is None:
            algebra = CGA3DAlgebra()
        super().__init__(algebra)

        # Cache product table organized by result index
        self._products_by_result = self._organize_products_by_result()

    def _organize_products_by_result(self) -> Dict[int, List[Tuple[int, int, int]]]:
        """
        Organize product rules by result index for efficient code generation.

        Returns:
            Dict mapping result_idx -> [(left_idx, right_idx, sign), ...]
        """
        result = {k: [] for k in range(self.algebra.blade_count)}
        for (left, right), (res, sign) in self.algebra.get_product_table().items():
            result[res].append((left, right, sign))
        return result

    def generate_header(self) -> str:
        """Generate module header with imports and docstring."""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return f'''"""
CGA Cl(4,1) Functional Operations - Auto-generated

DO NOT EDIT MANUALLY - This file is generated by codegen/generate.py

Generated: {timestamp}
Algebra: {self.algebra.name}
Signature: {self.algebra.signature}
Blade count: {self.algebra.blade_count}

All functions are:
- Loop-free (fully expanded arithmetic)
- ONNX-compatible (only Add/Mul/Neg/Sub operations)
- Hard-coded (no Cayley table lookups)
"""

import torch
from torch import Tensor
from typing import Tuple

'''

    def generate_constants(self) -> str:
        """Generate constant definitions (T015)."""
        lines = [
            "# =============================================================================",
            "# Constants",
            "# =============================================================================",
            "",
            "BLADE_COUNT = 32",
            "",
            "# Blade indices by grade",
            f"GRADE_0_INDICES = {self.algebra.get_grade_indices(0)}",
            f"GRADE_1_INDICES = {self.algebra.get_grade_indices(1)}",
            f"GRADE_2_INDICES = {self.algebra.get_grade_indices(2)}",
            f"GRADE_3_INDICES = {self.algebra.get_grade_indices(3)}",
            f"GRADE_4_INDICES = {self.algebra.get_grade_indices(4)}",
            f"GRADE_5_INDICES = {self.algebra.get_grade_indices(5)}",
            "",
            "# Sparsity masks",
            "UPGC_POINT_MASK = GRADE_1_INDICES  # 5 components",
            "EVEN_VERSOR_MASK = GRADE_0_INDICES + GRADE_2_INDICES + GRADE_4_INDICES  # 16 components",
            "",
            "# Reverse signs for all 32 blades",
            f"REVERSE_SIGNS = {self.algebra.get_reverse_signs()}",
            "",
            "# EvenVersor-specific reverse signs (16 components)",
            "# Grade 0: +1, Grade 2: -1 (10 components), Grade 4: +1 (5 components)",
            "EVEN_VERSOR_REVERSE_SIGNS = (1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1)",
            "",
        ]
        return "\n".join(lines)

    def generate_geometric_product(self) -> str:
        """Generate the full 32x32 geometric product function (T016)."""
        lines = [
            "# =============================================================================",
            "# Geometric Product (Full 32x32)",
            "# =============================================================================",
            "",
            "@torch.jit.script",
            "def geometric_product_full(a: Tensor, b: Tensor) -> Tensor:",
            '    """',
            "    Compute the full geometric product of two multivectors.",
            "",
            "    Args:",
            "        a: Left operand, shape (..., 32)",
            "        b: Right operand, shape (..., 32)",
            "",
            "    Returns:",
            "        Result multivector, shape (..., 32)",
            "",
            "    Note:",
            "        Fully expanded, no loops, ONNX compatible.",
            '    """',
        ]

        # Generate computation for each result index
        for result_idx in range(self.algebra.blade_count):
            terms = self._products_by_result[result_idx]
            if not terms:
                lines.append(f"    # r{result_idx} = 0 (no contributions)")
                lines.append(f"    r{result_idx} = torch.zeros_like(a[..., 0])")
            else:
                # Build the sum of products
                term_strs = []
                for left, right, sign in terms:
                    if sign == 1:
                        term_strs.append(f"a[..., {left}] * b[..., {right}]")
                    else:
                        term_strs.append(f"-a[..., {left}] * b[..., {right}]")

                # Split into multiple lines if too many terms
                if len(term_strs) <= 3:
                    lines.append(f"    r{result_idx} = {' + '.join(term_strs)}")
                else:
                    lines.append(f"    r{result_idx} = (")
                    for i, term in enumerate(term_strs):
                        if i < len(term_strs) - 1:
                            lines.append(f"        {term} +")
                        else:
                            lines.append(f"        {term}")
                    lines.append("    )")

        # Stack all results
        lines.append("")
        lines.append("    return torch.stack([")
        for i in range(0, 32, 8):
            chunk = ", ".join(f"r{j}" for j in range(i, min(i + 8, 32)))
            lines.append(f"        {chunk},")
        lines.append("    ], dim=-1)")
        lines.append("")

        return "\n".join(lines)

    def generate_reverse(self) -> str:
        """Generate the reverse operation function (T017)."""
        signs = self.algebra.get_reverse_signs()

        lines = [
            "# =============================================================================",
            "# Reverse Operation",
            "# =============================================================================",
            "",
            "@torch.jit.script",
            "def reverse_full(mv: Tensor) -> Tensor:",
            '    """',
            "    Compute the reverse of a multivector.",
            "",
            "    For grade k: coefficient *= (-1)^(k*(k-1)/2)",
            "    Grade 0, 1, 4, 5: sign = +1",
            "    Grade 2, 3: sign = -1",
            "",
            "    Args:",
            "        mv: Input multivector, shape (..., 32)",
            "",
            "    Returns:",
            "        Reversed multivector, shape (..., 32)",
            '    """',
        ]

        # Generate each component
        for idx in range(32):
            sign = signs[idx]
            if sign == 1:
                lines.append(f"    r{idx} = mv[..., {idx}]")
            else:
                lines.append(f"    r{idx} = -mv[..., {idx}]")

        # Stack results
        lines.append("")
        lines.append("    return torch.stack([")
        for i in range(0, 32, 8):
            chunk = ", ".join(f"r{j}" for j in range(i, min(i + 8, 32)))
            lines.append(f"        {chunk},")
        lines.append("    ], dim=-1)")
        lines.append("")

        return "\n".join(lines)

    def generate_sparse_section(self) -> str:
        """Generate sparse operation functions (T025, T026, T030)."""
        from .sparse_analysis import (
            get_sandwich_product_terms,
            EVEN_VERSOR_FULL_INDICES,
            UPGC_POINT_FULL_INDICES,
            EVEN_VERSOR_PATTERN,
            UPGC_POINT_PATTERN,
        )

        # Get all terms for sandwich product
        terms = get_sandwich_product_terms(
            self.algebra.get_product_table(),
            EVEN_VERSOR_FULL_INDICES,
            UPGC_POINT_FULL_INDICES,
            self.algebra.get_reverse_signs()
        )

        lines = [
            "# =============================================================================",
            "# Sparse Operations (EvenVersor x Point)",
            "# =============================================================================",
            "",
            "# EvenVersor sparse indices: Grade 0 (1) + Grade 2 (10) + Grade 4 (5) = 16",
            "# Point sparse indices: Grade 1 (5)",
            "",
        ]

        # Generate upgc_encode (T026)
        lines.extend(self._generate_upgc_encode())
        lines.append("")

        # Generate upgc_decode (T026)
        lines.extend(self._generate_upgc_decode())
        lines.append("")

        # Generate reverse_even_versor (T030)
        lines.extend(self._generate_reverse_even_versor())
        lines.append("")

        # Generate sandwich_product_sparse (T025)
        lines.extend(self._generate_sandwich_product_sparse(terms))
        lines.append("")

        return "\n".join(lines)

    def _generate_upgc_encode(self) -> List[str]:
        """Generate UPGC encode function."""
        return [
            "@torch.jit.script",
            "def upgc_encode(x: Tensor) -> Tensor:",
            '    """',
            "    Encode 3D vector to UPGC point representation.",
            "",
            "    X = n_o + x + 0.5|x|^2 * n_inf",
            "",
            "    Where:",
            "        n_o = 0.5 * (e- - e+)   -> coefficients: e+ = -0.5, e- = 0.5",
            "        n_inf = e- + e+         -> coefficients: e+ = 1, e- = 1",
            "",
            "    Args:",
            "        x: 3D vector, shape (..., 3)",
            "",
            "    Returns:",
            "        UPGC point, shape (..., 5) as [e1, e2, e3, e+, e-]",
            '    """',
            "    x1 = x[..., 0]",
            "    x2 = x[..., 1]",
            "    x3 = x[..., 2]",
            "",
            "    # |x|^2 / 2",
            "    half_norm_sq = 0.5 * (x1 * x1 + x2 * x2 + x3 * x3)",
            "",
            "    # e1, e2, e3 components are just x",
            "    r0 = x1  # e1",
            "    r1 = x2  # e2",
            "    r2 = x3  # e3",
            "",
            "    # e+ component: -0.5 (from n_o) + half_norm_sq * 1 (from n_inf)",
            "    r3 = -0.5 + half_norm_sq  # e+",
            "",
            "    # e- component: 0.5 (from n_o) + half_norm_sq * 1 (from n_inf)",
            "    r4 = 0.5 + half_norm_sq  # e-",
            "",
            "    return torch.stack([r0, r1, r2, r3, r4], dim=-1)",
            "",
        ]

    def _generate_upgc_decode(self) -> List[str]:
        """Generate UPGC decode function."""
        return [
            "@torch.jit.script",
            "def upgc_decode(point: Tensor) -> Tensor:",
            '    """',
            "    Decode UPGC point to 3D vector.",
            "",
            "    Extracts the e1, e2, e3 components directly.",
            "",
            "    Args:",
            "        point: UPGC point, shape (..., 5) as [e1, e2, e3, e+, e-]",
            "",
            "    Returns:",
            "        3D vector, shape (..., 3)",
            '    """',
            "    return point[..., :3]",
            "",
        ]

    def _generate_reverse_even_versor(self) -> List[str]:
        """Generate reverse_even_versor function (T030)."""
        # EvenVersor reverse signs: Grade 0 (+1), Grade 2 (-1 x 10), Grade 4 (+1 x 5)
        return [
            "@torch.jit.script",
            "def reverse_even_versor(ev: Tensor) -> Tensor:",
            '    """',
            "    Compute reverse of an EvenVersor (sparse 16-component version).",
            "",
            "    EvenVersor layout (16 components):",
            "        [0]: scalar (Grade 0) -> +1",
            "        [1-10]: bivectors (Grade 2) -> -1",
            "        [11-15]: quadvectors (Grade 4) -> +1",
            "",
            "    Args:",
            "        ev: EvenVersor, shape (..., 16)",
            "",
            "    Returns:",
            "        Reversed EvenVersor, shape (..., 16)",
            '    """',
            "    # Grade 0: keep sign",
            "    r0 = ev[..., 0]",
            "",
            "    # Grade 2 (10 components): negate",
            "    r1 = -ev[..., 1]",
            "    r2 = -ev[..., 2]",
            "    r3 = -ev[..., 3]",
            "    r4 = -ev[..., 4]",
            "    r5 = -ev[..., 5]",
            "    r6 = -ev[..., 6]",
            "    r7 = -ev[..., 7]",
            "    r8 = -ev[..., 8]",
            "    r9 = -ev[..., 9]",
            "    r10 = -ev[..., 10]",
            "",
            "    # Grade 4 (5 components): keep sign",
            "    r11 = ev[..., 11]",
            "    r12 = ev[..., 12]",
            "    r13 = ev[..., 13]",
            "    r14 = ev[..., 14]",
            "    r15 = ev[..., 15]",
            "",
            "    return torch.stack([",
            "        r0, r1, r2, r3, r4, r5, r6, r7, r8, r9, r10, r11, r12, r13, r14, r15",
            "    ], dim=-1)",
            "",
        ]

    def _generate_sandwich_product_sparse(
        self,
        terms: dict
    ) -> List[str]:
        """Generate sandwich_product_sparse function (T025)."""
        from .sparse_analysis import UPGC_POINT_FULL_INDICES, count_multiplication_ops

        lines = [
            "@torch.jit.script",
            "def sandwich_product_sparse(ev: Tensor, point: Tensor) -> Tensor:",
            '    """',
            "    Compute sparse sandwich product: M × X × M̃",
            "",
            "    Optimized for:",
            "        - EvenVersor M: 16 components (Grade 0, 2, 4)",
            "        - Point X: 5 components (Grade 1)",
            "        - Output: 5 components (Grade 1)",
            "",
            "    Args:",
            "        ev: EvenVersor, shape (..., 16)",
            "               [scalar, e12, e13, e1+, e1-, e23, e2+, e2-, e3+, e3-, e+-,",
            "                e123+, e123-, e12+-, e13+-, e23+-]",
            "        point: UPGC point, shape (..., 5)",
            "               [e1, e2, e3, e+, e-]",
            "",
            "    Returns:",
            "        Transformed point, shape (..., 5)",
            f"",
            f"    Note:",
            f"        Total multiplications: {count_multiplication_ops(terms)} (vs 2048 for full)",
            '    """',
            "    # EvenVersor components (sparse)",
            "    m0 = ev[..., 0]    # scalar",
            "    m1 = ev[..., 1]    # e12",
            "    m2 = ev[..., 2]    # e13",
            "    m3 = ev[..., 3]    # e1+",
            "    m4 = ev[..., 4]    # e1-",
            "    m5 = ev[..., 5]    # e23",
            "    m6 = ev[..., 6]    # e2+",
            "    m7 = ev[..., 7]    # e2-",
            "    m8 = ev[..., 8]    # e3+",
            "    m9 = ev[..., 9]    # e3-",
            "    m10 = ev[..., 10]  # e+-",
            "    m11 = ev[..., 11]  # e123+",
            "    m12 = ev[..., 12]  # e123-",
            "    m13 = ev[..., 13]  # e12+-",
            "    m14 = ev[..., 14]  # e13+-",
            "    m15 = ev[..., 15]  # e23+-",
            "",
            "    # Point components (sparse)",
            "    p0 = point[..., 0]  # e1",
            "    p1 = point[..., 1]  # e2",
            "    p2 = point[..., 2]  # e3",
            "    p3 = point[..., 3]  # e+",
            "    p4 = point[..., 4]  # e-",
            "",
            "    # EvenVersor reverse (Grade 2 negated)",
            "    mr0 = m0",
            "    mr1 = -m1",
            "    mr2 = -m2",
            "    mr3 = -m3",
            "    mr4 = -m4",
            "    mr5 = -m5",
            "    mr6 = -m6",
            "    mr7 = -m7",
            "    mr8 = -m8",
            "    mr9 = -m9",
            "    mr10 = -m10",
            "    mr11 = m11",
            "    mr12 = m12",
            "    mr13 = m13",
            "    mr14 = m14",
            "    mr15 = m15",
            "",
        ]

        # Generate output for each of the 5 point components
        for out_idx, full_idx in enumerate(UPGC_POINT_FULL_INDICES):
            component_terms = terms.get(full_idx, [])

            if not component_terms:
                lines.append(f"    # Output r{out_idx} (blade {full_idx}): no terms")
                lines.append(f"    r{out_idx} = torch.zeros_like(p0)")
            else:
                lines.append(f"    # Output r{out_idx} (blade {full_idx}): {len(component_terms)} terms")
                lines.append(f"    r{out_idx} = (")

                # Generate each term
                for i, (m_i, p_j, m_l, sign) in enumerate(component_terms):
                    sign_str = "" if sign == 1 else "-"
                    term = f"{sign_str}m{m_i} * p{p_j} * mr{m_l}"
                    if i < len(component_terms) - 1:
                        lines.append(f"        {term} +")
                    else:
                        lines.append(f"        {term}")
                lines.append("    )")
            lines.append("")

        # Return stacked result
        lines.append("    return torch.stack([r0, r1, r2, r3, r4], dim=-1)")
        lines.append("")

        return lines

    def generate_module(self) -> str:
        """Generate the complete functional.py module."""
        parts = [
            self.generate_header(),
            self.generate_constants(),
            self.generate_geometric_product(),
            self.generate_reverse(),
            self.generate_sparse_section(),
        ]
        return "\n".join(parts)


def generate_cga3d_functional(output_path: str) -> None:
    """
    Generate the CGA3D functional module.

    Args:
        output_path: Path to write the generated code
    """
    generator = CGA3DCodeGenerator()
    code = generator.generate_module()

    with open(output_path, 'w') as f:
        f.write(code)

    print(f"Generated: {output_path}")
    print(f"  - Constants: BLADE_COUNT, GRADE_*_INDICES, REVERSE_SIGNS")
    print(f"  - Functions: geometric_product_full, reverse_full")


class CGANDAlgebra(AlgebraDefinition):
    """
    通用化 CGA Cl(n+1,1) 代數定義。

    支援任意歐幾里得維度的 CGA 代數。
    """

    def __init__(self, euclidean_dim: int):
        """
        初始化 CGA 代數定義。

        Args:
            euclidean_dim: 歐幾里得空間維度 (1, 2, 或 3)
        """
        self._euclidean_dim = euclidean_dim
        self._blade_count = compute_blade_count(euclidean_dim)
        self._grade_indices = compute_grade_indices(euclidean_dim)
        self._product_table = get_product_table(euclidean_dim)
        self._reverse_signs = compute_reverse_signs(euclidean_dim)
        self._blade_names = get_blade_names(euclidean_dim)
        self._upgc_point_indices = get_upgc_point_indices(euclidean_dim)
        self._even_versor_indices = get_even_versor_indices(euclidean_dim)

    @property
    def euclidean_dim(self) -> int:
        return self._euclidean_dim

    @property
    def name(self) -> str:
        return f"cga{self._euclidean_dim}d"

    @property
    def signature(self) -> Tuple[int, ...]:
        # (+,...,+,-) 共 n+2 個元素
        return tuple([1] * (self._euclidean_dim + 1) + [-1])

    @property
    def blade_count(self) -> int:
        return self._blade_count

    def get_grade_indices(self, grade: int) -> Tuple[int, ...]:
        return self._grade_indices.get(grade, ())

    def get_product_table(self) -> Dict[Tuple[int, int], Tuple[int, int]]:
        return self._product_table

    def get_reverse_signs(self) -> Tuple[int, ...]:
        return self._reverse_signs

    def get_blade_names(self) -> List[str]:
        return self._blade_names

    def get_upgc_point_indices(self) -> Tuple[int, ...]:
        return self._upgc_point_indices

    def get_even_versor_indices(self) -> Tuple[int, ...]:
        return self._even_versor_indices


class CGANDCodeGenerator(CodeGenerator):
    """
    通用化 CGA Cl(n+1,1) 代碼生成器。

    生成 PyTorch 代碼，特性：
    - 硬編碼算術（無 Cayley 表查找）
    - 無循環（完全展開）
    - 僅 ONNX 相容操作
    """

    def __init__(self, euclidean_dim: int):
        """
        初始化代碼生成器。

        Args:
            euclidean_dim: 歐幾里得空間維度 (1, 2, 或 3)
        """
        algebra = CGANDAlgebra(euclidean_dim)
        super().__init__(algebra)
        self._euclidean_dim = euclidean_dim

        # 快取按結果索引組織的乘積規則
        self._products_by_result = self._organize_products_by_result()

    @property
    def euclidean_dim(self) -> int:
        return self._euclidean_dim

    def _organize_products_by_result(self) -> Dict[int, List[Tuple[int, int, int]]]:
        """
        按結果索引組織乘積規則。

        Returns:
            Dict 映射 result_idx -> [(left_idx, right_idx, sign), ...]
        """
        result = {k: [] for k in range(self.algebra.blade_count)}
        for (left, right), (res, sign) in self.algebra.get_product_table().items():
            result[res].append((left, right, sign))
        return result

    def generate_header(self) -> str:
        """生成模組標頭。"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        dim = self._euclidean_dim
        total_dim = dim + 2

        return f'''"""
CGA{dim}D Cl({dim+1},1) Functional Operations - Auto-generated

DO NOT EDIT MANUALLY - This file is generated by codegen/generate.py

Generated: {timestamp}
Algebra: {self.algebra.name}
Signature: {self.algebra.signature}
Blade count: {self.algebra.blade_count}

All functions are:
- Loop-free (fully expanded arithmetic)
- ONNX-compatible (only Add/Mul/Neg/Sub operations)
- Hard-coded (no Cayley table lookups)
"""

import torch
from torch import Tensor
from typing import Tuple

'''

    def generate_constants(self) -> str:
        """生成常數定義。"""
        dim = self._euclidean_dim
        total_dim = dim + 2
        blade_count = self.algebra.blade_count

        lines = [
            "# =============================================================================",
            "# Constants",
            "# =============================================================================",
            "",
            f"BLADE_COUNT = {blade_count}",
            f"EUCLIDEAN_DIM = {dim}",
            "",
            "# Blade indices by grade",
        ]

        # 生成各 grade 索引
        for grade in range(total_dim + 1):
            indices = self.algebra.get_grade_indices(grade)
            if indices:
                lines.append(f"GRADE_{grade}_INDICES = {indices}")

        lines.append("")
        lines.append("# Sparsity masks")

        # UPGC Point 遮罩
        upgc_indices = self.algebra.get_upgc_point_indices()
        lines.append(f"UPGC_POINT_MASK = {upgc_indices}  # {len(upgc_indices)} components")

        # EvenVersor 遮罩
        even_versor_indices = self.algebra.get_even_versor_indices()
        lines.append(f"EVEN_VERSOR_MASK = {even_versor_indices}  # {len(even_versor_indices)} components")

        lines.append("")
        lines.append(f"# Reverse signs for all {blade_count} blades")
        lines.append(f"REVERSE_SIGNS = {self.algebra.get_reverse_signs()}")

        # EvenVersor reverse 符號
        lines.append("")
        lines.append(f"# EvenVersor-specific reverse signs ({len(even_versor_indices)} components)")
        even_versor_reverse_signs = []
        for idx in even_versor_indices:
            even_versor_reverse_signs.append(self.algebra.get_reverse_signs()[idx])
        lines.append(f"EVEN_VERSOR_REVERSE_SIGNS = {tuple(even_versor_reverse_signs)}")

        lines.append("")
        return "\n".join(lines)

    def generate_geometric_product(self) -> str:
        """生成完整幾何積函數。"""
        blade_count = self.algebra.blade_count

        lines = [
            "# =============================================================================",
            f"# Geometric Product (Full {blade_count}x{blade_count})",
            "# =============================================================================",
            "",
            "@torch.jit.script",
            "def geometric_product_full(a: Tensor, b: Tensor) -> Tensor:",
            '    """',
            "    Compute the full geometric product of two multivectors.",
            "",
            "    Args:",
            f"        a: Left operand, shape (..., {blade_count})",
            f"        b: Right operand, shape (..., {blade_count})",
            "",
            "    Returns:",
            f"        Result multivector, shape (..., {blade_count})",
            "",
            "    Note:",
            "        Fully expanded, no loops, ONNX compatible.",
            '    """',
        ]

        # 為每個結果索引生成計算
        for result_idx in range(blade_count):
            terms = self._products_by_result[result_idx]
            if not terms:
                lines.append(f"    # r{result_idx} = 0 (no contributions)")
                lines.append(f"    r{result_idx} = torch.zeros_like(a[..., 0])")
            else:
                term_strs = []
                for left, right, sign in terms:
                    if sign == 1:
                        term_strs.append(f"a[..., {left}] * b[..., {right}]")
                    else:
                        term_strs.append(f"-a[..., {left}] * b[..., {right}]")

                if len(term_strs) <= 3:
                    lines.append(f"    r{result_idx} = {' + '.join(term_strs)}")
                else:
                    lines.append(f"    r{result_idx} = (")
                    for i, term in enumerate(term_strs):
                        if i < len(term_strs) - 1:
                            lines.append(f"        {term} +")
                        else:
                            lines.append(f"        {term}")
                    lines.append("    )")

        # Stack 所有結果
        lines.append("")
        lines.append("    return torch.stack([")

        # 按每 8 個分組
        chunk_size = 8
        for i in range(0, blade_count, chunk_size):
            chunk = ", ".join(f"r{j}" for j in range(i, min(i + chunk_size, blade_count)))
            lines.append(f"        {chunk},")
        lines.append("    ], dim=-1)")
        lines.append("")

        return "\n".join(lines)

    def generate_reverse(self) -> str:
        """生成 reverse 操作函數。"""
        signs = self.algebra.get_reverse_signs()
        blade_count = self.algebra.blade_count

        lines = [
            "# =============================================================================",
            "# Reverse Operation",
            "# =============================================================================",
            "",
            "@torch.jit.script",
            "def reverse_full(mv: Tensor) -> Tensor:",
            '    """',
            "    Compute the reverse of a multivector.",
            "",
            "    For grade k: coefficient *= (-1)^(k*(k-1)/2)",
            "",
            "    Args:",
            f"        mv: Input multivector, shape (..., {blade_count})",
            "",
            "    Returns:",
            f"        Reversed multivector, shape (..., {blade_count})",
            '    """',
        ]

        for idx in range(blade_count):
            sign = signs[idx]
            if sign == 1:
                lines.append(f"    r{idx} = mv[..., {idx}]")
            else:
                lines.append(f"    r{idx} = -mv[..., {idx}]")

        lines.append("")
        lines.append("    return torch.stack([")
        chunk_size = 8
        for i in range(0, blade_count, chunk_size):
            chunk = ", ".join(f"r{j}" for j in range(i, min(i + chunk_size, blade_count)))
            lines.append(f"        {chunk},")
        lines.append("    ], dim=-1)")
        lines.append("")

        return "\n".join(lines)

    def generate_sparse_section(self) -> str:
        """生成稀疏操作函數。"""
        from .sparse_analysis import (
            get_even_versor_pattern,
            get_upgc_point_pattern,
            get_sandwich_product_terms_generic,
            count_multiplication_ops,
        )

        dim = self._euclidean_dim
        even_versor_pattern = get_even_versor_pattern(dim)
        point_pattern = get_upgc_point_pattern(dim)
        terms = get_sandwich_product_terms_generic(dim)
        mul_ops = count_multiplication_ops(terms)

        even_versor_count = even_versor_pattern.sparse_count
        point_count = point_pattern.sparse_count
        even_versor_indices = even_versor_pattern.nonzero_indices
        point_indices = point_pattern.nonzero_indices

        # 取得 blade 名稱
        blade_names = self.algebra.get_blade_names()

        lines = [
            "# =============================================================================",
            f"# Sparse Operations (EvenVersor[{even_versor_count}] x Point[{point_count}])",
            "# =============================================================================",
            "",
            f"# EvenVersor sparse indices: {even_versor_indices}",
            f"# Point sparse indices: {point_indices}",
            "",
        ]

        # 生成 upgc_encode
        lines.extend(self._generate_upgc_encode(dim, point_indices, blade_names))
        lines.append("")

        # 生成 upgc_decode
        lines.extend(self._generate_upgc_decode(dim, point_count))
        lines.append("")

        # 生成 reverse_even_versor
        lines.extend(self._generate_reverse_even_versor(even_versor_pattern, blade_names))
        lines.append("")

        # 生成 sandwich_product_sparse
        lines.extend(self._generate_sandwich_product_sparse(
            terms, even_versor_pattern, point_pattern, blade_names, mul_ops
        ))
        lines.append("")

        return "\n".join(lines)

    def _generate_upgc_encode(
        self, dim: int, point_indices: Tuple[int, ...], blade_names: List[str]
    ) -> List[str]:
        """生成 UPGC encode 函數。"""
        point_count = len(point_indices)

        lines = [
            "@torch.jit.script",
            "def upgc_encode(x: Tensor) -> Tensor:",
            '    """',
            f"    Encode {dim}D vector to UPGC point representation.",
            "",
            "    X = n_o + x + 0.5|x|^2 * n_inf",
            "",
            "    Where:",
            "        n_o = 0.5 * (e- - e+)   -> coefficients: e+ = -0.5, e- = 0.5",
            "        n_inf = e- + e+         -> coefficients: e+ = 1, e- = 1",
            "",
            "    Args:",
            f"        x: {dim}D vector, shape (..., {dim})",
            "",
            "    Returns:",
            f"        UPGC point, shape (..., {point_count})",
            '    """',
        ]

        if dim == 0:
            # CGA0D 特殊處理：輸入 batch_size 作為形狀參考
            lines.append("    # CGA0D: no euclidean components")
            lines.append("    # x is expected to be shape (..., 0) - use for batch shape")
            lines.append("    batch_shape = x.shape[:-1]")
            lines.append("    r0 = torch.full(batch_shape, -0.5, dtype=x.dtype, device=x.device)  # e+")
            lines.append("    r1 = torch.full(batch_shape, 0.5, dtype=x.dtype, device=x.device)  # e-")
        else:
            # 提取各分量
            for i in range(dim):
                lines.append(f"    x{i+1} = x[..., {i}]")
            lines.append("")

            # 計算 |x|^2 / 2
            norm_terms = " + ".join(f"x{i+1} * x{i+1}" for i in range(dim))
            lines.append(f"    half_norm_sq = 0.5 * ({norm_terms})")
            lines.append("")

            # 歐幾里得分量
            for i in range(dim):
                lines.append(f"    r{i} = x{i+1}  # {blade_names[point_indices[i]]}")

            # e+ 和 e- 分量
            # e+ = -0.5 + half_norm_sq
            # e- = 0.5 + half_norm_sq
            lines.append(f"    r{dim} = -0.5 + half_norm_sq  # e+")
            lines.append(f"    r{dim+1} = 0.5 + half_norm_sq  # e-")

        lines.append("")
        result_vars = ", ".join(f"r{i}" for i in range(point_count))
        lines.append(f"    return torch.stack([{result_vars}], dim=-1)")
        lines.append("")

        return lines

    def _generate_upgc_decode(self, dim: int, point_count: int) -> List[str]:
        """生成 UPGC decode 函數。"""
        return [
            "@torch.jit.script",
            "def upgc_decode(point: Tensor) -> Tensor:",
            '    """',
            f"    Decode UPGC point to {dim}D vector.",
            "",
            f"    Extracts the first {dim} components (euclidean part).",
            "",
            "    Args:",
            f"        point: UPGC point, shape (..., {point_count})",
            "",
            "    Returns:",
            f"        {dim}D vector, shape (..., {dim})",
            '    """',
            f"    return point[..., :{dim}]",
            "",
        ]

    def _generate_reverse_even_versor(
        self, even_versor_pattern: SparsityPattern, blade_names: List[str]
    ) -> List[str]:
        """生成 reverse_even_versor 函數。"""
        even_versor_count = even_versor_pattern.sparse_count
        even_versor_indices = even_versor_pattern.nonzero_indices
        reverse_signs = self.algebra.get_reverse_signs()

        lines = [
            "@torch.jit.script",
            "def reverse_even_versor(ev: Tensor) -> Tensor:",
            '    """',
            f"    Compute reverse of an EvenVersor (sparse {even_versor_count}-component version).",
            "",
            "    Args:",
            f"        ev: EvenVersor, shape (..., {even_versor_count})",
            "",
            "    Returns:",
            f"        Reversed EvenVersor, shape (..., {even_versor_count})",
            '    """',
        ]

        # 對每個 even_versor 分量
        for sparse_idx, full_idx in enumerate(even_versor_indices):
            sign = reverse_signs[full_idx]
            name = blade_names[full_idx]
            if sign == 1:
                lines.append(f"    r{sparse_idx} = ev[..., {sparse_idx}]  # {name}: keep")
            else:
                lines.append(f"    r{sparse_idx} = -ev[..., {sparse_idx}]  # {name}: negate")

        lines.append("")
        result_vars = ", ".join(f"r{i}" for i in range(even_versor_count))
        lines.append(f"    return torch.stack([{result_vars}], dim=-1)")
        lines.append("")

        return lines

    def _generate_sandwich_product_sparse(
        self,
        terms: dict,
        even_versor_pattern: SparsityPattern,
        point_pattern: SparsityPattern,
        blade_names: List[str],
        mul_ops: int
    ) -> List[str]:
        """生成 sandwich_product_sparse 函數。"""
        even_versor_count = even_versor_pattern.sparse_count
        point_count = point_pattern.sparse_count
        even_versor_indices = even_versor_pattern.nonzero_indices
        point_indices = point_pattern.nonzero_indices
        reverse_signs = self.algebra.get_reverse_signs()

        # 計算理論最大乘法數
        full_ops = even_versor_count * point_count * even_versor_count * 2

        lines = [
            "@torch.jit.script",
            "def sandwich_product_sparse(ev: Tensor, point: Tensor) -> Tensor:",
            '    """',
            "    Compute sparse sandwich product: M × X × M̃",
            "",
            f"    Optimized for:",
            f"        - EvenVersor M: {even_versor_count} components",
            f"        - Point X: {point_count} components",
            f"        - Output: {point_count} components",
            "",
            "    Args:",
            f"        ev: EvenVersor, shape (..., {even_versor_count})",
            f"        point: UPGC point, shape (..., {point_count})",
            "",
            "    Returns:",
            f"        Transformed point, shape (..., {point_count})",
            "",
            f"    Note:",
            f"        Total multiplications: {mul_ops} (vs {full_ops} for naive)",
            '    """',
        ]

        # EvenVersor 分量
        lines.append("    # EvenVersor components (sparse)")
        for sparse_idx, full_idx in enumerate(even_versor_indices):
            name = blade_names[full_idx]
            lines.append(f"    m{sparse_idx} = ev[..., {sparse_idx}]  # {name}")
        lines.append("")

        # Point 分量
        lines.append("    # Point components (sparse)")
        for sparse_idx, full_idx in enumerate(point_indices):
            name = blade_names[full_idx]
            lines.append(f"    p{sparse_idx} = point[..., {sparse_idx}]  # {name}")
        lines.append("")

        # EvenVersor reverse
        lines.append("    # EvenVersor reverse")
        for sparse_idx, full_idx in enumerate(even_versor_indices):
            sign = reverse_signs[full_idx]
            if sign == 1:
                lines.append(f"    mr{sparse_idx} = m{sparse_idx}")
            else:
                lines.append(f"    mr{sparse_idx} = -m{sparse_idx}")
        lines.append("")

        # 對每個輸出分量生成計算
        for out_idx, full_idx in enumerate(point_indices):
            component_terms = terms.get(full_idx, [])

            if not component_terms:
                lines.append(f"    # Output r{out_idx} ({blade_names[full_idx]}): no terms")
                lines.append(f"    r{out_idx} = torch.zeros_like(p0)")
            else:
                lines.append(f"    # Output r{out_idx} ({blade_names[full_idx]}): {len(component_terms)} terms")
                lines.append(f"    r{out_idx} = (")

                for i, (m_i, p_j, m_l, sign) in enumerate(component_terms):
                    sign_str = "" if sign == 1 else "-"
                    term = f"{sign_str}m{m_i} * p{p_j} * mr{m_l}"
                    if i < len(component_terms) - 1:
                        lines.append(f"        {term} +")
                    else:
                        lines.append(f"        {term}")
                lines.append("    )")
            lines.append("")

        # 返回堆疊結果
        result_vars = ", ".join(f"r{i}" for i in range(point_count))
        lines.append(f"    return torch.stack([{result_vars}], dim=-1)")
        lines.append("")

        return lines

    # =========================================================================
    # T012: _generate_compose_even_versor - EvenVersor 組合代碼生成
    # =========================================================================

    def _generate_compose_even_versor(self) -> str:
        """
        生成 compose_even_versor 函數 (T012)。

        EvenVersor × EvenVersor = EvenVersor
        """
        dim = self._euclidean_dim
        terms = get_compose_even_versor_terms(dim)
        even_versor_indices = self.algebra.get_even_versor_indices()
        even_versor_count = len(even_versor_indices)
        blade_names = self.algebra.get_blade_names()

        lines = [
            "# =============================================================================",
            "# EvenVersor Composition",
            "# =============================================================================",
            "",
            "@torch.jit.script",
            "def compose_even_versor(v1: Tensor, v2: Tensor) -> Tensor:",
            '    """',
            "    Compose two EvenVersors via geometric product.",
            "",
            "    Args:",
            f"        v1: First EvenVersor, shape (..., {even_versor_count})",
            f"        v2: Second EvenVersor, shape (..., {even_versor_count})",
            "",
            "    Returns:",
            f"        Composed EvenVersor, shape (..., {even_versor_count})",
            '    """',
        ]

        # 提取 v1 和 v2 的各分量
        lines.append("    # v1 components")
        for sparse_idx, full_idx in enumerate(even_versor_indices):
            name = blade_names[full_idx]
            lines.append(f"    v1_{sparse_idx} = v1[..., {sparse_idx}]  # {name}")

        lines.append("")
        lines.append("    # v2 components")
        for sparse_idx, full_idx in enumerate(even_versor_indices):
            name = blade_names[full_idx]
            lines.append(f"    v2_{sparse_idx} = v2[..., {sparse_idx}]  # {name}")

        lines.append("")

        # 生成各輸出分量的計算
        for out_sparse_idx in range(even_versor_count):
            out_full_idx = even_versor_indices[out_sparse_idx]
            out_name = blade_names[out_full_idx]
            component_terms = terms.get(out_sparse_idx, [])

            if not component_terms:
                lines.append(f"    # r{out_sparse_idx} ({out_name}): no terms")
                lines.append(f"    r{out_sparse_idx} = torch.zeros_like(v1_0)")
            else:
                lines.append(f"    # r{out_sparse_idx} ({out_name}): {len(component_terms)} terms")
                if len(component_terms) <= 3:
                    term_strs = []
                    for v1_i, v2_j, sign in component_terms:
                        if sign == 1:
                            term_strs.append(f"v1_{v1_i} * v2_{v2_j}")
                        else:
                            term_strs.append(f"-v1_{v1_i} * v2_{v2_j}")
                    lines.append(f"    r{out_sparse_idx} = {' + '.join(term_strs)}")
                else:
                    lines.append(f"    r{out_sparse_idx} = (")
                    for i, (v1_i, v2_j, sign) in enumerate(component_terms):
                        sign_str = "" if sign == 1 else "-"
                        term = f"{sign_str}v1_{v1_i} * v2_{v2_j}"
                        if i < len(component_terms) - 1:
                            lines.append(f"        {term} +")
                        else:
                            lines.append(f"        {term}")
                    lines.append("    )")

        lines.append("")
        result_vars = ", ".join(f"r{i}" for i in range(even_versor_count))
        lines.append(f"    return torch.stack([{result_vars}], dim=-1)")
        lines.append("")

        return "\n".join(lines)

    # =========================================================================
    # T013: _generate_compose_similitude - Similitude 組合代碼生成
    # =========================================================================

    def _generate_compose_similitude(self) -> str:
        """
        生成 compose_similitude 函數 (T013)。

        Similitude × Similitude = Similitude (利用約束加速)
        """
        dim = self._euclidean_dim
        terms = get_compose_similitude_terms(dim)
        even_versor_indices = self.algebra.get_even_versor_indices()
        even_versor_count = len(even_versor_indices)
        blade_names = self.algebra.get_blade_names()

        lines = [
            "# =============================================================================",
            "# Similitude Composition (CGA-specific accelerated)",
            "# =============================================================================",
            "",
            "@torch.jit.script",
            "def compose_similitude(s1: Tensor, s2: Tensor) -> Tensor:",
            '    """',
            "    Compose two Similitudes via optimized geometric product.",
            "",
            "    Similitude constraint: ei+ = ei- (translation components equal)",
            "    This allows 30-50% speedup over general EvenVersor composition.",
            "",
            "    Args:",
            f"        s1: First Similitude, shape (..., {even_versor_count})",
            f"        s2: Second Similitude, shape (..., {even_versor_count})",
            "",
            "    Returns:",
            f"        Composed Similitude, shape (..., {even_versor_count})",
            '    """',
            "    # Note: Currently same as compose_even_versor",
            "    # Future optimization: exploit ei+ = ei- constraint",
        ]

        # 提取分量（與 EvenVersor 相同）
        lines.append("    # s1 components")
        for sparse_idx, full_idx in enumerate(even_versor_indices):
            name = blade_names[full_idx]
            lines.append(f"    s1_{sparse_idx} = s1[..., {sparse_idx}]  # {name}")

        lines.append("")
        lines.append("    # s2 components")
        for sparse_idx, full_idx in enumerate(even_versor_indices):
            name = blade_names[full_idx]
            lines.append(f"    s2_{sparse_idx} = s2[..., {sparse_idx}]  # {name}")

        lines.append("")

        # 生成各輸出分量
        for out_sparse_idx in range(even_versor_count):
            out_full_idx = even_versor_indices[out_sparse_idx]
            out_name = blade_names[out_full_idx]
            component_terms = terms.get(out_sparse_idx, [])

            if not component_terms:
                lines.append(f"    # r{out_sparse_idx} ({out_name}): no terms")
                lines.append(f"    r{out_sparse_idx} = torch.zeros_like(s1_0)")
            else:
                lines.append(f"    # r{out_sparse_idx} ({out_name}): {len(component_terms)} terms")
                if len(component_terms) <= 3:
                    term_strs = []
                    for s1_i, s2_j, sign in component_terms:
                        if sign == 1:
                            term_strs.append(f"s1_{s1_i} * s2_{s2_j}")
                        else:
                            term_strs.append(f"-s1_{s1_i} * s2_{s2_j}")
                    lines.append(f"    r{out_sparse_idx} = {' + '.join(term_strs)}")
                else:
                    lines.append(f"    r{out_sparse_idx} = (")
                    for i, (s1_i, s2_j, sign) in enumerate(component_terms):
                        sign_str = "" if sign == 1 else "-"
                        term = f"{sign_str}s1_{s1_i} * s2_{s2_j}"
                        if i < len(component_terms) - 1:
                            lines.append(f"        {term} +")
                        else:
                            lines.append(f"        {term}")
                    lines.append("    )")

        lines.append("")
        result_vars = ", ".join(f"r{i}" for i in range(even_versor_count))
        lines.append(f"    return torch.stack([{result_vars}], dim=-1)")
        lines.append("")

        return "\n".join(lines)

    # =========================================================================
    # T014: _generate_sandwich_product_similitude - Similitude 三明治積代碼生成
    # =========================================================================

    def _generate_sandwich_product_similitude(self) -> str:
        """
        生成 sandwich_product_similitude 函數 (T014)。

        S × X × ~S for Similitude (faster than EvenVersor)
        """
        dim = self._euclidean_dim
        even_versor_indices = self.algebra.get_even_versor_indices()
        even_versor_count = len(even_versor_indices)
        point_indices = self.algebra.get_upgc_point_indices()
        point_count = len(point_indices)

        lines = [
            "# =============================================================================",
            "# Similitude Sandwich Product (CGA-specific accelerated)",
            "# =============================================================================",
            "",
            "@torch.jit.script",
            "def sandwich_product_similitude(similitude: Tensor, point: Tensor) -> Tensor:",
            '    """',
            "    Compute sandwich product S × X × ~S for Similitude.",
            "",
            "    Faster than sandwich_product_sparse by exploiting Similitude constraints.",
            "",
            "    Args:",
            f"        similitude: Similitude, shape (..., {even_versor_count})",
            f"        point: UPGC point, shape (..., {point_count})",
            "",
            "    Returns:",
            f"        Transformed point, shape (..., {point_count})",
            '    """',
            "    # Currently delegates to sandwich_product_sparse",
            "    # Future optimization: exploit ei+ = ei- constraint",
            "    return sandwich_product_sparse(similitude, point)",
            "",
        ]

        return "\n".join(lines)

    # =========================================================================
    # T015: _generate_inner_product_full - 幾何內積代碼生成
    # =========================================================================

    def _generate_inner_product_full(self) -> str:
        """
        生成 inner_product_full 函數 (T015)。

        <a * b>_0 = sum(a[i] * b[i] * metric_sign[i])
        """
        dim = self._euclidean_dim
        blade_count = self.algebra.blade_count
        inner_signs = get_inner_product_signs(dim)
        blade_names = self.algebra.get_blade_names()

        lines = [
            "# =============================================================================",
            "# Geometric Inner Product (Metric Inner Product)",
            "# =============================================================================",
            "",
            "@torch.jit.script",
            "def inner_product_full(a: Tensor, b: Tensor) -> Tensor:",
            '    """',
            "    Compute geometric inner product with CGA metric.",
            "",
            "    Args:",
            f"        a: First multivector, shape (..., {blade_count})",
            f"        b: Second multivector, shape (..., {blade_count})",
            "",
            "    Returns:",
            "        Scalar inner product, shape (..., 1)",
            "",
            "    Note:",
            "        Uses fused sign flipping: res = sum(a[i] * b[i] * METRIC_SIGNS[i])",
            '    """',
        ]

        # 收集非零符號的項
        nonzero_terms = [(i, sign) for i, sign in enumerate(inner_signs) if sign != 0]

        if not nonzero_terms:
            lines.append("    return torch.zeros_like(a[..., 0:1])")
        else:
            lines.append("    r = (")
            for i, (idx, sign) in enumerate(nonzero_terms):
                name = blade_names[idx]
                if sign == 1:
                    term = f"a[..., {idx}] * b[..., {idx}]"
                else:
                    term = f"-a[..., {idx}] * b[..., {idx}]"
                comment = f"  # {name}² = {sign}"
                if i < len(nonzero_terms) - 1:
                    lines.append(f"        {term} +{comment}")
                else:
                    lines.append(f"        {term}{comment}")
            lines.append("    )")
            lines.append("    return r.unsqueeze(-1)")

        lines.append("")
        return "\n".join(lines)

    # =========================================================================
    # T016: _generate_bivector_squared_scalar - Bivector 平方標量輔助函數
    # =========================================================================

    def _generate_bivector_squared_scalar(self) -> str:
        """
        生成 bivector_squared_scalar 輔助函數 (T016)。

        B² 的標量部分，用於 exp_bivector
        """
        dim = self._euclidean_dim
        terms = get_bivector_squared_terms(dim)
        biv_indices = get_bivector_indices(dim)
        biv_count = len(biv_indices)
        blade_names = self.algebra.get_blade_names()

        lines = [
            "# =============================================================================",
            "# Bivector Squared Scalar (Helper for exp_bivector)",
            "# =============================================================================",
            "",
            "@torch.jit.script",
            "def bivector_squared_scalar(B: Tensor) -> Tensor:",
            '    """',
            "    Compute the scalar part of B².",
            "",
            "    Args:",
            f"        B: Bivector, shape (..., {biv_count})",
            "",
            "    Returns:",
            "        Scalar B² value, shape (...,)",
            '    """',
        ]

        if not terms:
            lines.append("    return torch.zeros_like(B[..., 0])")
        else:
            lines.append("    r = (")
            for i, (bi, bj, sign) in enumerate(terms):
                sign_str = "" if sign == 1 else "-"
                term = f"{sign_str}B[..., {bi}] * B[..., {bj}]"
                if i < len(terms) - 1:
                    lines.append(f"        {term} +")
                else:
                    lines.append(f"        {term}")
            lines.append("    )")
            lines.append("    return r")

        lines.append("")
        return "\n".join(lines)

    # =========================================================================
    # T017: _generate_exp_bivector - Bivector 指數映射代碼生成
    # =========================================================================

    def _generate_exp_bivector(self) -> str:
        """
        生成 exp_bivector 函數 (T017)。

        exp(B) = cos(θ) + sinc(θ) * θ * B/θ = cos(θ) + sinc(θ) * B
        where θ² = -B²
        """
        dim = self._euclidean_dim
        biv_indices = get_bivector_indices(dim)
        biv_count = len(biv_indices)
        even_versor_indices = self.algebra.get_even_versor_indices()
        even_versor_count = len(even_versor_indices)
        blade_names = self.algebra.get_blade_names()
        grade_indices = compute_grade_indices(dim)

        # 找出 even_versor 中 bivector 對應的稀疏索引
        # even_versor = [scalar, ...bivectors..., ...quadvectors...]
        biv_to_ev_sparse = {}
        for ev_sparse, ev_full in enumerate(even_versor_indices):
            if ev_full in biv_indices:
                biv_sparse = biv_indices.index(ev_full)
                biv_to_ev_sparse[biv_sparse] = ev_sparse

        lines = [
            "# =============================================================================",
            "# Bivector Exponential Map",
            "# =============================================================================",
            "",
            "@torch.jit.script",
            "def exp_bivector(B: Tensor) -> Tensor:",
            '    """',
            "    Compute exponential map from Bivector to EvenVersor.",
            "",
            "    exp(B) = cos(θ) + sinc(θ) * B",
            "    where θ² = -B² (for rotation bivectors)",
            "",
            "    Args:",
            f"        B: Bivector, shape (..., {biv_count})",
            "",
            "    Returns:",
            f"        EvenVersor, shape (..., {even_versor_count})",
            "",
            "    Note:",
            "        Uses torch.sinc for numerical stability at θ→0",
            '    """',
            "    # Compute B² scalar",
            "    B_sq = bivector_squared_scalar(B)",
            "",
            "    # θ² = -B² (for rotation bivectors, B² is negative)",
            "    theta_sq = torch.clamp(-B_sq, min=1e-12)",
            "    theta = torch.sqrt(theta_sq)",
            "",
            "    # cos(θ) and sinc(θ) = sin(θ)/θ",
            "    cos_theta = torch.cos(theta)",
            "    # torch.sinc(x) = sin(πx)/(πx), so sinc(θ/π) = sin(θ)/θ",
            "    sinc_theta = torch.sinc(theta / 3.141592653589793)",
            "",
            "    # Build even_versor: scalar part = cos(θ), bivector parts = sinc(θ) * B",
        ]

        # 生成各 even_versor 分量
        for ev_sparse, ev_full in enumerate(even_versor_indices):
            name = blade_names[ev_full]
            if ev_full == 0:  # scalar
                lines.append(f"    r{ev_sparse} = cos_theta  # {name}")
            elif ev_full in biv_indices:
                biv_sparse = biv_indices.index(ev_full)
                lines.append(f"    r{ev_sparse} = sinc_theta * B[..., {biv_sparse}]  # {name}")
            else:
                # quadvector: 0 for pure rotation
                lines.append(f"    r{ev_sparse} = torch.zeros_like(cos_theta)  # {name}")

        lines.append("")
        result_vars = ", ".join(f"r{i}" for i in range(even_versor_count))
        lines.append(f"    return torch.stack([{result_vars}], dim=-1)")
        lines.append("")

        return "\n".join(lines)

    # =========================================================================
    # T018: _generate_outer_product_full - 楔積代碼生成
    # =========================================================================

    def _generate_outer_product_full(self) -> str:
        """
        生成 outer_product_full 函數 (T018)。

        a ∧ b = <a * b>_{|a| + |b|}
        """
        dim = self._euclidean_dim
        blade_count = self.algebra.blade_count
        terms = get_outer_product_terms(dim)
        blade_names = self.algebra.get_blade_names()

        lines = [
            "# =============================================================================",
            "# Outer Product (Wedge Product)",
            "# =============================================================================",
            "",
            "@torch.jit.script",
            "def outer_product_full(a: Tensor, b: Tensor) -> Tensor:",
            '    """',
            "    Compute outer product (wedge product).",
            "",
            "    a ∧ b = <a * b>_{grade(a) + grade(b)}",
            "",
            "    Args:",
            f"        a: First multivector, shape (..., {blade_count})",
            f"        b: Second multivector, shape (..., {blade_count})",
            "",
            "    Returns:",
            f"        Wedge product, shape (..., {blade_count})",
            '    """',
        ]

        # 生成各輸出分量
        for out_idx in range(blade_count):
            name = blade_names[out_idx]
            component_terms = terms.get(out_idx, [])

            if not component_terms:
                lines.append(f"    r{out_idx} = torch.zeros_like(a[..., 0])  # {name}")
            elif len(component_terms) <= 3:
                term_strs = []
                for a_i, b_j, sign in component_terms:
                    if sign == 1:
                        term_strs.append(f"a[..., {a_i}] * b[..., {b_j}]")
                    else:
                        term_strs.append(f"-a[..., {a_i}] * b[..., {b_j}]")
                lines.append(f"    r{out_idx} = {' + '.join(term_strs)}  # {name}")
            else:
                lines.append(f"    r{out_idx} = (  # {name}")
                for i, (a_i, b_j, sign) in enumerate(component_terms):
                    sign_str = "" if sign == 1 else "-"
                    term = f"{sign_str}a[..., {a_i}] * b[..., {b_j}]"
                    if i < len(component_terms) - 1:
                        lines.append(f"        {term} +")
                    else:
                        lines.append(f"        {term}")
                lines.append("    )")

        lines.append("")
        lines.append("    return torch.stack([")
        chunk_size = 8
        for i in range(0, blade_count, chunk_size):
            chunk = ", ".join(f"r{j}" for j in range(i, min(i + chunk_size, blade_count)))
            lines.append(f"        {chunk},")
        lines.append("    ], dim=-1)")
        lines.append("")

        return "\n".join(lines)

    # =========================================================================
    # T019: _generate_left_contraction_full - 左縮併代碼生成
    # =========================================================================

    def _generate_left_contraction_full(self) -> str:
        """
        生成 left_contraction_full 函數 (T019)。

        a ⌋ b = <a * b>_{|b| - |a|} if |a| <= |b|, else 0
        """
        dim = self._euclidean_dim
        blade_count = self.algebra.blade_count
        terms = get_left_contraction_terms(dim)
        blade_names = self.algebra.get_blade_names()

        lines = [
            "# =============================================================================",
            "# Left Contraction",
            "# =============================================================================",
            "",
            "@torch.jit.script",
            "def left_contraction_full(a: Tensor, b: Tensor) -> Tensor:",
            '    """',
            "    Compute left contraction.",
            "",
            "    a ⌋ b = <a * b>_{grade(b) - grade(a)} if grade(a) <= grade(b), else 0",
            "",
            "    Args:",
            f"        a: First multivector, shape (..., {blade_count})",
            f"        b: Second multivector, shape (..., {blade_count})",
            "",
            "    Returns:",
            f"        Left contraction result, shape (..., {blade_count})",
            '    """',
        ]

        # 生成各輸出分量
        for out_idx in range(blade_count):
            name = blade_names[out_idx]
            component_terms = terms.get(out_idx, [])

            if not component_terms:
                lines.append(f"    r{out_idx} = torch.zeros_like(a[..., 0])  # {name}")
            elif len(component_terms) <= 3:
                term_strs = []
                for a_i, b_j, sign in component_terms:
                    if sign == 1:
                        term_strs.append(f"a[..., {a_i}] * b[..., {b_j}]")
                    else:
                        term_strs.append(f"-a[..., {a_i}] * b[..., {b_j}]")
                lines.append(f"    r{out_idx} = {' + '.join(term_strs)}  # {name}")
            else:
                lines.append(f"    r{out_idx} = (  # {name}")
                for i, (a_i, b_j, sign) in enumerate(component_terms):
                    sign_str = "" if sign == 1 else "-"
                    term = f"{sign_str}a[..., {a_i}] * b[..., {b_j}]"
                    if i < len(component_terms) - 1:
                        lines.append(f"        {term} +")
                    else:
                        lines.append(f"        {term}")
                lines.append("    )")

        lines.append("")
        lines.append("    return torch.stack([")
        chunk_size = 8
        for i in range(0, blade_count, chunk_size):
            chunk = ", ".join(f"r{j}" for j in range(i, min(i + chunk_size, blade_count)))
            lines.append(f"        {chunk},")
        lines.append("    ], dim=-1)")
        lines.append("")

        return "\n".join(lines)

    # =========================================================================
    # T020: _generate_right_contraction_full - 右縮併代碼生成
    # =========================================================================

    def _generate_right_contraction_full(self) -> str:
        """
        生成 right_contraction_full 函數 (T020)。

        a ⌊ b = <a * b>_{|a| - |b|} if |a| >= |b|, else 0
        """
        dim = self._euclidean_dim
        blade_count = self.algebra.blade_count
        terms = get_right_contraction_terms(dim)
        blade_names = self.algebra.get_blade_names()

        lines = [
            "# =============================================================================",
            "# Right Contraction",
            "# =============================================================================",
            "",
            "@torch.jit.script",
            "def right_contraction_full(a: Tensor, b: Tensor) -> Tensor:",
            '    """',
            "    Compute right contraction.",
            "",
            "    a ⌊ b = <a * b>_{grade(a) - grade(b)} if grade(a) >= grade(b), else 0",
            "",
            "    Args:",
            f"        a: First multivector, shape (..., {blade_count})",
            f"        b: Second multivector, shape (..., {blade_count})",
            "",
            "    Returns:",
            f"        Right contraction result, shape (..., {blade_count})",
            '    """',
        ]

        # 生成各輸出分量
        for out_idx in range(blade_count):
            name = blade_names[out_idx]
            component_terms = terms.get(out_idx, [])

            if not component_terms:
                lines.append(f"    r{out_idx} = torch.zeros_like(a[..., 0])  # {name}")
            elif len(component_terms) <= 3:
                term_strs = []
                for a_i, b_j, sign in component_terms:
                    if sign == 1:
                        term_strs.append(f"a[..., {a_i}] * b[..., {b_j}]")
                    else:
                        term_strs.append(f"-a[..., {a_i}] * b[..., {b_j}]")
                lines.append(f"    r{out_idx} = {' + '.join(term_strs)}  # {name}")
            else:
                lines.append(f"    r{out_idx} = (  # {name}")
                for i, (a_i, b_j, sign) in enumerate(component_terms):
                    sign_str = "" if sign == 1 else "-"
                    term = f"{sign_str}a[..., {a_i}] * b[..., {b_j}]"
                    if i < len(component_terms) - 1:
                        lines.append(f"        {term} +")
                    else:
                        lines.append(f"        {term}")
                lines.append("    )")

        lines.append("")
        lines.append("    return torch.stack([")
        chunk_size = 8
        for i in range(0, blade_count, chunk_size):
            chunk = ", ".join(f"r{j}" for j in range(i, min(i + chunk_size, blade_count)))
            lines.append(f"        {chunk},")
        lines.append("    ], dim=-1)")
        lines.append("")

        return "\n".join(lines)

    # =========================================================================
    # T021: _generate_grade_select - Grade 選擇代碼生成
    # =========================================================================

    def _generate_grade_select(self) -> str:
        """
        生成 grade_select 函數 (T021)。

        <a>_k = 提取 grade k 分量
        """
        dim = self._euclidean_dim
        blade_count = self.algebra.blade_count
        grade_indices = compute_grade_indices(dim)
        max_grade = dim + 2
        blade_names = self.algebra.get_blade_names()

        lines = [
            "# =============================================================================",
            "# Grade Selection",
            "# =============================================================================",
            "",
            "# Grade masks for grade selection",
        ]

        # 生成 grade masks 常數
        for grade in range(max_grade + 1):
            indices = grade_indices.get(grade, ())
            lines.append(f"GRADE_{grade}_MASK = {indices}")

        lines.append("")
        lines.append("@torch.jit.script")
        lines.append("def grade_select(mv: Tensor, grade: int) -> Tensor:")
        lines.append('    """')
        lines.append("    Extract components of a specific grade.")
        lines.append("")
        lines.append("    Args:")
        lines.append(f"        mv: Multivector, shape (..., {blade_count})")
        lines.append(f"        grade: Grade to extract (0 to {max_grade})")
        lines.append("")
        lines.append("    Returns:")
        lines.append("        Grade components (zeros for non-selected grades)")
        lines.append('    """')

        # 使用 if-elif 鏈（ONNX 相容）
        for grade in range(max_grade + 1):
            indices = grade_indices.get(grade, ())
            if grade == 0:
                lines.append(f"    if grade == {grade}:")
            else:
                lines.append(f"    elif grade == {grade}:")

            # 生成各分量
            for out_idx in range(blade_count):
                if out_idx in indices:
                    lines.append(f"        r{out_idx} = mv[..., {out_idx}]")
                else:
                    lines.append(f"        r{out_idx} = torch.zeros_like(mv[..., 0])")

            lines.append("        return torch.stack([")
            chunk_size = 8
            for i in range(0, blade_count, chunk_size):
                chunk = ", ".join(f"r{j}" for j in range(i, min(i + chunk_size, blade_count)))
                lines.append(f"            {chunk},")
            lines.append("        ], dim=-1)")

        # else: 無效 grade
        lines.append("    else:")
        lines.append("        return torch.zeros_like(mv)")

        lines.append("")
        return "\n".join(lines)

    # =========================================================================
    # T022: _generate_dual - 對偶代碼生成
    # =========================================================================

    def _generate_dual(self) -> str:
        """
        生成 dual 函數 (T022)。

        a* = a × I^{-1} = a ⌋ I^{-1}
        """
        dim = self._euclidean_dim
        blade_count = self.algebra.blade_count
        pseudoscalar_info = get_pseudoscalar_info(dim)
        blade_names = self.algebra.get_blade_names()
        product_table = get_product_table(dim)

        ps_idx = pseudoscalar_info['index']
        ps_square = pseudoscalar_info['square']

        lines = [
            "# =============================================================================",
            "# Dual Operation",
            "# =============================================================================",
            "",
            f"# Pseudoscalar index: {ps_idx}, I² = {ps_square}",
            f"# I^{{-1}} = I / I² = I * {1/ps_square if ps_square != 0 else 'undefined'}",
            "",
            "@torch.jit.script",
            "def dual(mv: Tensor) -> Tensor:",
            '    """',
            "    Compute the dual of a multivector.",
            "",
            "    a* = a ⌋ I^{-1} (left contraction with pseudoscalar inverse)",
            "",
            "    Args:",
            f"        mv: Multivector, shape (..., {blade_count})",
            "",
            "    Returns:",
            f"        Dual multivector, shape (..., {blade_count})",
            '    """',
        ]

        if ps_square == 0:
            lines.append("    # Pseudoscalar is null, dual undefined")
            lines.append("    return torch.zeros_like(mv)")
        else:
            # 計算 a × I^{-1}
            # 對於每個輸入 blade a[i]，計算 blade[i] × I^{-1}
            # I^{-1} = I / I² = I * (1/I²)
            inv_ps_factor = 1.0 / ps_square

            # 建立輸出映射：a[i] × I → 哪個輸出 blade，符號是什麼
            dual_map = {}  # input_idx -> (output_idx, sign)
            for in_idx in range(blade_count):
                if (in_idx, ps_idx) in product_table:
                    out_idx, sign = product_table[(in_idx, ps_idx)]
                    # 乘以 I^{-1} = I / I²
                    dual_map[in_idx] = (out_idx, sign * inv_ps_factor)

            # 按輸出索引組織
            output_terms = {k: [] for k in range(blade_count)}
            for in_idx, (out_idx, factor) in dual_map.items():
                output_terms[out_idx].append((in_idx, factor))

            # 生成代碼
            for out_idx in range(blade_count):
                name = blade_names[out_idx]
                terms = output_terms[out_idx]

                if not terms:
                    lines.append(f"    r{out_idx} = torch.zeros_like(mv[..., 0])  # {name}")
                elif len(terms) == 1:
                    in_idx, factor = terms[0]
                    if factor == 1.0:
                        lines.append(f"    r{out_idx} = mv[..., {in_idx}]  # {name}")
                    elif factor == -1.0:
                        lines.append(f"    r{out_idx} = -mv[..., {in_idx}]  # {name}")
                    else:
                        lines.append(f"    r{out_idx} = {factor} * mv[..., {in_idx}]  # {name}")
                else:
                    term_strs = []
                    for in_idx, factor in terms:
                        if factor == 1.0:
                            term_strs.append(f"mv[..., {in_idx}]")
                        elif factor == -1.0:
                            term_strs.append(f"-mv[..., {in_idx}]")
                        else:
                            term_strs.append(f"{factor} * mv[..., {in_idx}]")
                    lines.append(f"    r{out_idx} = {' + '.join(term_strs)}  # {name}")

            lines.append("")
            lines.append("    return torch.stack([")
            chunk_size = 8
            for i in range(0, blade_count, chunk_size):
                chunk = ", ".join(f"r{j}" for j in range(i, min(i + chunk_size, blade_count)))
                lines.append(f"        {chunk},")
            lines.append("    ], dim=-1)")

        lines.append("")
        return "\n".join(lines)

    # =========================================================================
    # T023: _generate_normalize - 正規化代碼生成
    # =========================================================================

    def _generate_normalize(self) -> str:
        """
        生成 normalize 函數 (T023)。

        a / |a| where |a|² = <a * ~a>_0
        """
        dim = self._euclidean_dim
        blade_count = self.algebra.blade_count
        norm_terms = get_norm_squared_terms(dim)

        lines = [
            "# =============================================================================",
            "# Normalize Operation",
            "# =============================================================================",
            "",
            "@torch.jit.script",
            "def norm_squared(mv: Tensor) -> Tensor:",
            '    """',
            "    Compute squared norm of a multivector.",
            "",
            "    |a|² = <a * ~a>_0",
            "",
            "    Args:",
            f"        mv: Multivector, shape (..., {blade_count})",
            "",
            "    Returns:",
            "        Squared norm, shape (...,)",
            '    """',
        ]

        if not norm_terms:
            lines.append("    return torch.zeros_like(mv[..., 0])")
        else:
            lines.append("    r = (")
            for i, (idx, sign) in enumerate(norm_terms):
                if sign == 1:
                    term = f"mv[..., {idx}] * mv[..., {idx}]"
                else:
                    term = f"-mv[..., {idx}] * mv[..., {idx}]"
                if i < len(norm_terms) - 1:
                    lines.append(f"        {term} +")
                else:
                    lines.append(f"        {term}")
            lines.append("    )")
            lines.append("    return r")

        lines.append("")
        lines.append("")
        lines.append("@torch.jit.script")
        lines.append("def normalize(mv: Tensor) -> Tensor:")
        lines.append('    """')
        lines.append("    Normalize a multivector to unit norm.")
        lines.append("")
        lines.append("    Args:")
        lines.append(f"        mv: Multivector, shape (..., {blade_count})")
        lines.append("")
        lines.append("    Returns:")
        lines.append(f"        Normalized multivector, shape (..., {blade_count})")
        lines.append("")
        lines.append("    Note:")
        lines.append("        Returns NaN for null multivectors (ONNX compatible)")
        lines.append('    """')
        lines.append("    norm = torch.sqrt(torch.abs(norm_squared(mv)) + 1e-12)")
        lines.append("    return mv / norm.unsqueeze(-1)")
        lines.append("")

        return "\n".join(lines)

    # =========================================================================
    # T153k: _generate_structure_normalize - Structure Normalize for Similitude
    # =========================================================================

    def _generate_structure_normalize(self) -> str:
        """
        生成 structure_normalize 函式 (US9a)。

        Structure normalize 用於對 Similitude 進行結構正規化：
        1. 正規化 Rotor 部分（保持旋轉為單位四元數）
        2. 強制 Similitude 約束（eie+ = eie-，排除 transversion）
        """
        from .sparse_analysis import (
            get_rotor_indices,
            get_translation_pairs,
            get_dilation_index,
        )

        even_versor_count = len(self.algebra.get_even_versor_indices())
        rotor_indices = get_rotor_indices(self.euclidean_dim)
        translation_pairs = get_translation_pairs(self.euclidean_dim)
        dilation_idx = get_dilation_index(self.euclidean_dim)

        lines = [
            "# =============================================================================",
            "# Structure Normalize (US9a)",
            "# =============================================================================",
            "",
            f"ROTOR_INDICES: Tuple[int, ...] = {rotor_indices}",
            f"TRANSLATION_PAIRS: Tuple[Tuple[int, int], ...] = {tuple(translation_pairs)}",
            f"DILATION_INDEX: int = {dilation_idx}",
            "",
            "",
            "@torch.jit.script",
            "def structure_normalize(similitude: Tensor, eps: float = 1e-8) -> Tensor:",
            '    """',
            "    Structure-normalize a Similitude tensor.",
            "",
            "    Ensures:",
            "    1. Rotor part (scalar + spatial bivectors) has unit norm",
            "    2. Similitude constraint: eie+ = eie- (no transversion)",
            "",
            "    Args:",
            f"        similitude: Similitude tensor, shape (..., {even_versor_count})",
            "        eps: Numerical stability constant",
            "",
            "    Returns:",
            f"        Structure-normalized Similitude, shape (..., {even_versor_count})",
            '    """',
            "    result = similitude.clone()",
            "",
            "    # Step 1: Normalize Rotor part",
        ]

        if len(rotor_indices) > 0:
            # Build rotor norm squared calculation
            rotor_norm_terms = [f"similitude[..., {idx}] * similitude[..., {idx}]" for idx in rotor_indices]
            lines.append(f"    rotor_norm_sq = {' + '.join(rotor_norm_terms)}")
            lines.append("    rotor_norm = torch.sqrt(rotor_norm_sq + eps)")
            for idx in rotor_indices:
                lines.append(f"    result[..., {idx}] = similitude[..., {idx}] / rotor_norm")
        else:
            lines.append("    # No rotor components for this dimension")

        lines.append("")
        lines.append("    # Step 2: Enforce Similitude constraint (eie+ = eie-)")

        if len(translation_pairs) > 0:
            for plus_idx, minus_idx in translation_pairs:
                lines.append(f"    avg_{plus_idx} = (result[..., {plus_idx}] + result[..., {minus_idx}]) / 2")
                lines.append(f"    result[..., {plus_idx}] = avg_{plus_idx}")
                lines.append(f"    result[..., {minus_idx}] = avg_{plus_idx}")
        else:
            lines.append("    # No translation pairs for this dimension")

        lines.append("")
        lines.append("    return result")
        lines.append("")
        lines.append("")
        lines.append("@torch.jit.script")
        lines.append("def soft_structure_normalize(similitude: Tensor, strength: float = 1.0, eps: float = 1e-8) -> Tensor:")
        lines.append('    """')
        lines.append("    Soft structure normalization with interpolation.")
        lines.append("")
        lines.append("    Args:")
        lines.append(f"        similitude: Similitude tensor, shape (..., {even_versor_count})")
        lines.append("        strength: Interpolation strength (0=no change, 1=full normalize)")
        lines.append("        eps: Numerical stability constant")
        lines.append("")
        lines.append("    Returns:")
        lines.append(f"        Soft-normalized Similitude, shape (..., {even_versor_count})")
        lines.append('    """')
        lines.append("    normalized = structure_normalize(similitude, eps)")
        lines.append("    return similitude + strength * (normalized - similitude)")
        lines.append("")

        return "\n".join(lines)

    # =========================================================================
    # T024: 更新 generate_module - 整合所有新操作
    # =========================================================================

    def generate_extended_operations(self) -> str:
        """
        生成所有擴展操作 (T024)。

        整合 T012-T023 的所有新操作，加上 T153k structure_normalize。
        """
        parts = [
            self._generate_compose_even_versor(),
            self._generate_compose_similitude(),
            self._generate_sandwich_product_similitude(),
            self._generate_inner_product_full(),
            self._generate_bivector_squared_scalar(),
            self._generate_exp_bivector(),
            self._generate_outer_product_full(),
            self._generate_left_contraction_full(),
            self._generate_right_contraction_full(),
            self._generate_grade_select(),
            self._generate_dual(),
            self._generate_normalize(),
            self._generate_structure_normalize(),
        ]
        return "\n".join(parts)

    def generate_module(self) -> str:
        """生成完整的 functional.py 模組。"""
        parts = [
            self.generate_header(),
            self.generate_constants(),
            self.generate_geometric_product(),
            self.generate_reverse(),
            self.generate_sparse_section(),
            self.generate_extended_operations(),
        ]
        return "\n".join(parts)


def generate_cgand_functional(euclidean_dim: int, output_path: str) -> None:
    """
    生成指定維度的 CGA functional 模組。

    Args:
        euclidean_dim: 歐幾里得空間維度 (1, 2, 或 3)
        output_path: 輸出檔案路徑
    """
    generator = CGANDCodeGenerator(euclidean_dim)
    code = generator.generate_module()

    with open(output_path, 'w') as f:
        f.write(code)

    print(f"Generated: {output_path}")
    print(f"  - Algebra: CGA{euclidean_dim}D Cl({euclidean_dim+1},1)")
    print(f"  - Blade count: {generator.algebra.blade_count}")
    print(f"  - EvenVersor components: {len(generator.algebra.get_even_versor_indices())}")
    print(f"  - Point components: {len(generator.algebra.get_upgc_point_indices())}")


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 2:
        dim = int(sys.argv[1])
        output_path = sys.argv[2]
        generate_cgand_functional(dim, output_path)
    elif len(sys.argv) > 1:
        generate_cga3d_functional(sys.argv[1])
    else:
        print("Usage:")
        print("  python -m fast_clifford.codegen.generate <dim> <output_path>")
        print("  python -m fast_clifford.codegen.generate <output_path>  # CGA3D only")
